# Configuraci√≥n de Ollama con MedGemma

## üìã Resumen de Cambios

Se ha configurado el backend para usar **Ollama** con el modelo `amsaravi/medgemma-4b-it:q8` en lugar de LM Studio.

## üèóÔ∏è Arquitectura del Backend

### Flujo Actual del Backend:

1. **Backend Principal**: `main.py` (FastAPI)
   - Puerto: `5001`
   - Endpoints: `/api/chat`, `/api/image-analysis`, `/api/history`, etc.

2. **Sistema LLM**: `langchain_system.py`
   - Usa LangChain con `ChatOpenAI` (compatible con API de OpenAI)
   - Se conecta a Ollama mediante su endpoint compatible con OpenAI API
   - Modelo: `amsaravi/medgemma-4b-it:q8`

3. **An√°lisis de Im√°genes**: `medical_analysis.py`
   - Usa httpx para hacer requests directos a Ollama
   - Soporta an√°lisis multimodal (texto + im√°genes)

4. **Variables de Entorno**: 
   - `OLLAMA_ENDPOINT`: URL del endpoint de Ollama (default: `http://localhost:11434/v1/`)
   - `LM_STUDIO_URL`: Compatible hacia atr√°s (se usa si no existe `OLLAMA_ENDPOINT`)

## ‚úÖ Cambios Realizados

### 1. `langchain_system.py`
- ‚úÖ Cambiado `FallbackLLM` para usar Ollama en lugar de LM Studio
- ‚úÖ Endpoint actualizado: `http://localhost:11434/v1/` (puerto est√°ndar de Ollama)
- ‚úÖ Modelo actualizado: `amsaravi/medgemma-4b-it:q8`
- ‚úÖ Todas las referencias a LM Studio reemplazadas por Ollama

### 2. `medical_analysis.py`
- ‚úÖ Configurado para usar Ollama
- ‚úÖ Endpoint: `OLLAMA_ENDPOINT` o `LM_STUDIO_URL` como fallback
- ‚úÖ Modelo: `amsaravi/medgemma-4b-it:q8`

### 3. `main.py`
- ‚úÖ Inicializaci√≥n de `medical_chain` ahora lee `OLLAMA_ENDPOINT` desde variables de entorno
- ‚úÖ Provider actualizado: `ollama` en lugar de `lm-studio`
- ‚úÖ Modelo actualizado en m√©tricas: `amsaravi/medgemma-4b-it:q8`

### 4. `env.local`
- ‚úÖ `OLLAMA_ENDPOINT=http://localhost:11434/v1/`
- ‚úÖ `LM_STUDIO_URL=http://localhost:11434` (compatibilidad)

## üöÄ C√≥mo Iniciar

### 1. Verificar que Ollama est√© instalado

```bash
ollama --version
```

### 2. Verificar que el modelo est√© disponible

```bash
ollama ls
```

Deber√≠as ver:
```
amsaravi/medgemma-4b-it:q8    905dc0be940a    5.0 GB
```

Si no est√° listado, desc√°rgalo:
```bash
ollama pull amsaravi/medgemma-4b-it:q8
```

### 3. Iniciar Ollama (si no est√° corriendo)

Ollama normalmente se ejecuta como servicio en segundo plano. Si necesitas iniciarlo manualmente:

```bash
ollama serve
```

Esto iniciar√° el servidor en `http://localhost:11434`

### 4. Verificar que Ollama est√© funcionando

```bash
curl http://localhost:11434/api/tags
```

O probar una consulta simple:
```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "amsaravi/medgemma-4b-it:q8",
    "messages": [
      {
        "role": "user",
        "content": "Hola, ¬øc√≥mo est√°s?"
      }
    ]
  }'
```

### 5. Iniciar el Backend

Desde el directorio ra√≠z del proyecto:

```bash
cd chatbot
python main.py
```

O usando el script de inicio:
```bash
./start-all.sh
```

## üîç Verificaci√≥n de la Conexi√≥n

### Logs del Backend

Al iniciar el backend, deber√≠as ver en los logs:

```
‚úÖ Configurado para usar Ollama local en: http://localhost:11434/v1/
‚úÖ Modelo: amsaravi/medgemma-4b-it:q8
```

### Probar el Chat

1. Abre el frontend (Gateway Next.js) en `http://localhost:3001` (o el puerto configurado)
2. Env√≠a un mensaje de prueba
3. Deber√≠as ver respuestas del modelo medgemma

### Probar An√°lisis de Im√°genes

1. Sube una imagen m√©dica (por ejemplo, una radiograf√≠a)
2. El sistema deber√≠a analizar la imagen usando Ollama con medgemma

## üêõ Troubleshooting

### Error: "Connection refused" o "Connection timeout"

**Problema**: Ollama no est√° corriendo o no est√° accesible

**Soluci√≥n**:
```bash
# Verificar si Ollama est√° corriendo
ps aux | grep ollama

# Iniciar Ollama si no est√° corriendo
ollama serve
```

### Error: "Model not found"

**Problema**: El modelo `amsaravi/medgemma-4b-it:q8` no est√° disponible

**Soluci√≥n**:
```bash
# Listar modelos disponibles
ollama ls

# Descargar el modelo si no est√° disponible
ollama pull amsaravi/medgemma-4b-it:q8
```

### Error: "Invalid endpoint"

**Problema**: La URL del endpoint no es correcta

**Soluci√≥n**: Verificar que `OLLAMA_ENDPOINT` termine con `/v1/`:
```bash
# En env.local o variables de entorno
OLLAMA_ENDPOINT=http://localhost:11434/v1/
```

### El backend sigue usando LM Studio

**Problema**: Variables de entorno no cargadas correctamente

**Soluci√≥n**:
1. Verificar que `env.local` existe en el directorio ra√≠z
2. Asegurar que las variables se carguen:
   ```bash
   export OLLAMA_ENDPOINT=http://localhost:11434/v1/
   ```
3. Reiniciar el backend

## üìù Notas Importantes

1. **Puerto de Ollama**: El puerto por defecto de Ollama es `11434`, diferente de LM Studio (`1234`)

2. **API Compatible**: Ollama es compatible con la API de OpenAI, por eso funciona con LangChain `ChatOpenAI`

3. **Modelo**: El modelo `amsaravi/medgemma-4b-it:q8` es una versi√≥n cuantizada (q8) del modelo MedGemma-4B

4. **Multimodal**: El modelo soporta an√°lisis de im√°genes (multimodal), por lo que el an√°lisis de radiograf√≠as deber√≠a funcionar

5. **Performance**: El modelo q8 es una versi√≥n cuantizada que balancea calidad y velocidad

## üîÑ Migraci√≥n desde LM Studio

Si antes usabas LM Studio:

1. ‚úÖ Ya no necesitas ejecutar LM Studio
2. ‚úÖ Ejecuta `ollama serve` en su lugar
3. ‚úÖ Actualiza las variables de entorno (`OLLAMA_ENDPOINT` en lugar de `LM_STUDIO_URL`)
4. ‚úÖ Descarga el modelo: `ollama pull amsaravi/medgemma-4b-it:q8`

## üìö Referencias

- [Documentaci√≥n de Ollama](https://ollama.ai/docs)
- [MedGemma en Ollama](https://ollama.ai/library/medgemma)
- [API de Ollama compatible con OpenAI](https://ollama.ai/docs/api)

