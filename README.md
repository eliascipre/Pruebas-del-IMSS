# ğŸ¥ Suite IMSS - Plataforma de Inteligencia Artificial MÃ©dica

## ğŸ“‹ DescripciÃ³n General

La **Suite IMSS** es una plataforma integral de Inteligencia Artificial mÃ©dica desarrollada para el Instituto Mexicano del Seguro Social (IMSS). Esta suite demuestra el potencial de la IA especializada en medicina para crear experiencias de aprendizaje interactivas, herramientas de apoyo clÃ­nico y simulaciones educativas avanzadas.

### ğŸ¯ **Objetivos Principales:**
- **EducaciÃ³n MÃ©dica Avanzada**: Simulaciones interactivas para estudiantes y profesionales
- **Apoyo ClÃ­nico**: Herramientas de anÃ¡lisis radiolÃ³gico con IA especializada
- **Aprendizaje Personalizado**: Entornos adaptativos basados en el progreso del usuario
- **IntegraciÃ³n de Modelos**: OrquestaciÃ³n de mÃºltiples modelos de IA mÃ©dica con vLLM y Ray Serve

---

## ğŸ—ï¸ Arquitectura Completa del Sistema

### **Arquitectura de Alto Nivel**

```mermaid
graph TB
    subgraph "ğŸŒ Capa de PresentaciÃ³n"
        GW[Next.js Gateway<br/>Puerto 3000/3001<br/>TypeScript + Tailwind]
    end
    
    subgraph "ğŸ”Œ Capa de Servicios Backend"
        CB[Chatbot MÃ©dico<br/>FastAPI - Puerto 5001<br/>LangChain + Streaming]
        ED[EducaciÃ³n RadiolÃ³gica<br/>Flask - Puerto 5002<br/>MedGemma Local]
        SM[SimulaciÃ³n Entrevistas<br/>Flask - Puerto 5003<br/>MedGemma + TTS]
        RX[RadiografÃ­as TÃ³rax<br/>Flask - Puerto 5004<br/>RAG + ChromaDB]
        NV[NV-Reason-CXR<br/>Gradio - Puerto 5005<br/>Modelo Especializado]
    end
    
    subgraph "ğŸ§  Capa de Inferencia vLLM + Ray Serve"
        direction TB
        subgraph "Cluster Ray Serve"
            MG27B[MedGemma 27B<br/>Puerto 8000<br/>4 GPUs - Tensor Parallel]
            NVR3B[NV-Reason-CXR 3B<br/>Puerto 8001<br/>2 GPUs - Tensor Parallel]
        end
        RS[Ray Serve<br/>Autoscaling<br/>2-16 rÃ©plicas MedGemma<br/>1-8 rÃ©plicas NV-Reason]
    end
    
    subgraph "ğŸ’¾ Capa de Almacenamiento"
        CD[ChromaDB<br/>Vector Store<br/>RAG Knowledge Base]
        SQ[SQLite<br/>Conversaciones<br/>Cache Local]
        FS[File System<br/>ImÃ¡genes/PDFs<br/>Media Storage]
    end
    
    GW --> CB
    GW --> ED
    GW --> SM
    GW --> RX
    GW --> NV
    
    CB --> MG27B
    CB --> CD
    CB --> SQ
    CB --> FS
    
    ED --> MG27B
    SM --> MG27B
    RX --> NVR3B
    RX --> CD
    NV --> NVR3B
    
    MG27B --> RS
    NVR3B --> RS
```

### **Flujo de Datos Detallado**

#### **1. Flujo de Consulta en Chatbot MÃ©dico**

```
Usuario â†’ Gateway (Next.js) 
    â†’ Chatbot API (FastAPI:5001)
    â†’ LangChain System (langchain_system.py)
    â†’ FallbackLLM (conexiÃ³n a vLLM)
    â†’ Ray Serve (puerto 8000)
    â†’ MedGemma 27B (4 GPUs)
    â†’ Streaming Response (SSE)
    â†’ Usuario
```

**Componentes Clave:**
- **FastAPI Backend** (`chatbot/main.py`): Maneja peticiones HTTP, streaming, autenticaciÃ³n
- **LangChain System** (`chatbot/langchain_system.py`): Orquesta la cadena de procesamiento
- **FallbackLLM**: Cliente HTTP optimizado con connection pooling, circuit breaker, backoff exponencial
- **vLLM Endpoint**: API compatible con OpenAI en `http://localhost:8000/v1/chat/completions`
- **Memoria Persistente**: SQLite para historial de conversaciones
- **AnÃ¡lisis de ImÃ¡genes**: Soporte multimodal con procesamiento de imÃ¡genes base64

#### **2. Flujo de AnÃ¡lisis RadiolÃ³gico**

```
Usuario â†’ Gateway
    â†’ Servicio RadiografÃ­as (Flask:5004)
    â†’ RAG System (ChromaDB)
    â†’ NV-Reason-CXR (Gradio:5005)
    â†’ vLLM Ray Serve (puerto 8001)
    â†’ NV-Reason-CXR 3B (2 GPUs)
    â†’ AnÃ¡lisis Especializado
    â†’ Usuario
```

#### **3. Flujo de EducaciÃ³n y SimulaciÃ³n**

```
Usuario â†’ Gateway
    â†’ EducaciÃ³n/SimulaciÃ³n (Flask:5002/5003)
    â†’ MedGemma Local Client
    â†’ vLLM Ray Serve (puerto 8000)
    â†’ MedGemma 27B
    â†’ Respuesta Educativa
    â†’ Usuario
```

---

## ğŸ§  Sistema de Inferencia vLLM + Ray Serve

### **Arquitectura de Inferencia Distribuida**

El proyecto utiliza **vLLM** con **Ray Serve** para proporcionar inferencia de alto rendimiento y escalable:

#### **ConfiguraciÃ³n de MedGemma 27B** (`vllm/serve_medgemma.py`)

```python
# ConfiguraciÃ³n del Motor vLLM
engine_args = AsyncEngineArgs(
    model="google/medgemma-27b-it",
    tensor_parallel_size=4,        # DistribuciÃ³n en 4 GPUs
    dtype="bfloat16",               # PrecisiÃ³n optimizada
    max_model_len=8192,             # Contexto mÃ¡ximo
    gpu_memory_utilization=0.90,   # Uso de VRAM
    enforce_eager=True,             # Modo eager para estabilidad
    trust_remote_code=True,
)

# Autoscaling de Ray Serve
autoscaling_config={
    "min_replicas": 2,              # MÃ­nimo 2 rÃ©plicas activas
    "max_replicas": 16,             # MÃ¡ximo 16 rÃ©plicas (64 GPUs)
    "target_ongoing_requests": 5    # Escala con 5+ peticiones en cola
}
```

**CaracterÃ­sticas:**
- **Tensor Parallelism**: Modelo distribuido en 4 GPUs para paralelismo de tensor
- **Autoscaling Inteligente**: Escala automÃ¡ticamente segÃºn la carga
- **API Compatible OpenAI**: Endpoint en `/v1/chat/completions`
- **Streaming Nativo**: Soporte para Server-Sent Events (SSE)
- **Multimodal**: Procesa texto e imÃ¡genes en formato base64

#### **ConfiguraciÃ³n de NV-Reason-CXR 3B** (`vllm/serve_nv_reason.py`)

```python
# ConfiguraciÃ³n optimizada para modelo 3B
engine_args = AsyncEngineArgs(
    model="nvidia/NV-Reason-CXR-3B",
    tensor_parallel_size=2,          # 2 GPUs suficientes para 3B
    dtype="bfloat16",
    max_model_len=8192,
    gpu_memory_utilization=0.90,
    enforce_eager=True,
    trust_remote_code=True,
)

# Autoscaling mÃ¡s conservador
autoscaling_config={
    "min_replicas": 1,              # 1 rÃ©plica mÃ­nima
    "max_replicas": 8,              # MÃ¡ximo 8 rÃ©plicas
    "target_ongoing_requests": 3   # Escala con 3+ peticiones
}
```

### **Inicio del Sistema vLLM**

```bash
# Iniciar Ray Serve con MedGemma 27B
cd /home/administrador/vllm
serve run serve_medgemma:medgemma_app \
  --host 0.0.0.0 \
  --port 8000

# Iniciar Ray Serve con NV-Reason-CXR 3B (en otro terminal o nodo)
serve run serve_nv_reason:nv_reason_app \
  --host 0.0.0.0 \
  --port 8001
```

### **IntegraciÃ³n con Servicios Backend**

Los servicios backend se conectan a vLLM usando la API compatible con OpenAI:

```python
# Ejemplo de conexiÃ³n desde LangChain
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="google/medgemma-27b",
    base_url="http://localhost:8000/v1/",  # Endpoint vLLM
    api_key="not-needed",                   # vLLM no requiere API key
    temperature=0.7,
    streaming=True
)
```

**Optimizaciones Implementadas:**
- **Connection Pooling**: Cliente HTTP singleton para reutilizar conexiones
- **Circuit Breaker**: ProtecciÃ³n contra sobrecarga del servidor
- **Backoff Exponencial**: Reintentos inteligentes con jitter
- **Timeout Adaptativo**: Timeouts basados en tamaÃ±o del prompt
- **Rate Limiting**: Respeta headers de rate limiting del servidor

---

## ğŸ“š Servicios Detallados

### 1. ğŸ¤– **Chatbot MÃ©dico** (`chatbot/`)

**TecnologÃ­a**: FastAPI + LangChain + vLLM + Ray Serve

**Puerto**: 5001

**Arquitectura Interna:**

```
chatbot/
â”œâ”€â”€ main.py                    # FastAPI app principal
â”œâ”€â”€ langchain_system.py      # Sistema LangChain con FallbackLLM
â”œâ”€â”€ memory_manager.py          # GestiÃ³n de memoria con SQLite
â”œâ”€â”€ medical_analysis.py        # AnÃ¡lisis de imÃ¡genes mÃ©dicas
â”œâ”€â”€ media_storage.py          # Almacenamiento de multimedia
â”œâ”€â”€ transcription_service.py  # TranscripciÃ³n de audio (Whisper)
â”œâ”€â”€ auth_manager.py           # AutenticaciÃ³n de usuarios
â”œâ”€â”€ security_llm.py           # ValidaciÃ³n de seguridad LLM
â””â”€â”€ prompts/                  # Prompts mÃ©dicos especializados
```

**CaracterÃ­sticas Principales:**

1. **Streaming de Respuestas (SSE)**
   - Respuestas en tiempo real usando Server-Sent Events
   - CancelaciÃ³n de generaciones activas
   - Manejo de errores robusto

2. **Memoria Persistente**
   - SQLite para historial de conversaciones
   - IntegraciÃ³n con LangChain ChatMessageHistory
   - ExtracciÃ³n automÃ¡tica de entidades mÃ©dicas

3. **AnÃ¡lisis Multimodal**
   - Procesamiento de imÃ¡genes mÃ©dicas (base64)
   - TranscripciÃ³n de audio con Whisper
   - AnÃ¡lisis especializado con MedGemma 27B

4. **Seguridad y OptimizaciÃ³n**
   - Rate limiting por usuario
   - ValidaciÃ³n de prompts (OWASP LLM Top 10)
   - Circuit breaker para protecciÃ³n del servidor
   - Optimizaciones de VRAM

**Endpoints Principales:**
- `POST /api/chat` - Chat con streaming
- `POST /api/image-analysis` - AnÃ¡lisis de imÃ¡genes
- `POST /api/transcribe` - TranscripciÃ³n de audio
- `GET /api/history` - Historial de conversaciones
- `POST /api/cancel` - Cancelar generaciÃ³n activa
- `GET /health` - Health check

**Variables de Entorno:**
```bash
VLLM_ENDPOINT=http://localhost:8000/v1/  # Endpoint vLLM Ray Serve
CORS_ORIGINS=http://localhost:3000       # OrÃ­genes permitidos
```

### 2. ğŸ“š **EducaciÃ³n RadiolÃ³gica** (`Educacion_radiografia/`)

**TecnologÃ­a**: Flask + MedGemma Local + ChromaDB

**Puerto**: 5002

**CaracterÃ­sticas:**
- Casos clÃ­nicos interactivos
- AnÃ¡lisis de radiografÃ­as con IA
- Sistema de retroalimentaciÃ³n
- Base de conocimiento mÃ©dica con RAG

**IntegraciÃ³n:**
- Usa cliente local de MedGemma (`local_llm_client.py`)
- Puede conectarse a vLLM endpoint si estÃ¡ disponible
- Cache inteligente para optimizar respuestas

### 3. ğŸ­ **SimulaciÃ³n de Entrevistas** (`Simulacion/`)

**TecnologÃ­a**: Flask + React + MedGemma + TTS (Gemini)

**Puerto**: 5003

**CaracterÃ­sticas:**
- Pacientes virtuales con condiciones especÃ­ficas
- Entrevistas con sÃ­ntesis de voz (Gemini TTS)
- EvaluaciÃ³n automÃ¡tica de respuestas
- MÃºltiples escenarios clÃ­nicos

**Flujo:**
1. Usuario selecciona condiciÃ³n mÃ©dica
2. Sistema genera paciente virtual
3. Entrevista interactiva con streaming
4. EvaluaciÃ³n automÃ¡tica del reporte mÃ©dico

### 4. ğŸ« **RadiografÃ­as de TÃ³rax** (`radiografias_torax/`)

**TecnologÃ­a**: Flask + React + RAG + ChromaDB

**Puerto**: 5004

**CaracterÃ­sticas:**
- AnÃ¡lisis especializado de radiografÃ­as de tÃ³rax
- Sistema RAG con guÃ­as mÃ©dicas
- Interfaz interactiva de aprendizaje
- Casos de estudio progresivos

**Arquitectura RAG:**
- ChromaDB como vector store
- Embeddings de documentos mÃ©dicos
- BÃºsqueda semÃ¡ntica de informaciÃ³n relevante
- Contexto enriquecido para el modelo

### 5. ğŸ”¬ **NV-Reason-CXR** (`nv-reason-cxr/`)

**TecnologÃ­a**: Gradio + NVIDIA NV-Reason-CXR

**Puerto**: 5005

**CaracterÃ­sticas:**
- AnÃ¡lisis paso a paso de radiografÃ­as
- ExplicaciÃ³n del razonamiento clÃ­nico
- Modelo especializado en tÃ³rax (3B parÃ¡metros)
- Interfaz web intuitiva con Gradio

**IntegraciÃ³n con vLLM:**
- Puede usar modelo local (transformers)
- O conectarse a vLLM Ray Serve en puerto 8001
- ConfiguraciÃ³n flexible mediante variables de entorno

### 6. ğŸŒ **Gateway Principal** (`UI_IMSS/`)

**TecnologÃ­a**: Next.js 16 + TypeScript + Tailwind CSS

**Puerto**: 3000/3001 (auto-detecta puerto disponible)

**CaracterÃ­sticas:**
- Dashboard principal unificado
- NavegaciÃ³n entre todos los servicios
- PÃ¡ginas informativas detalladas
- IntegraciÃ³n completa con backend

**Estructura:**
```
UI_IMSS/
â”œâ”€â”€ app/                      # App Router de Next.js
â”‚   â”œâ”€â”€ chat/                # PÃ¡gina de chat
â”‚   â”œâ”€â”€ home/                # Dashboard principal
â”‚   â””â”€â”€ ...
â”œâ”€â”€ components/               # Componentes React
â”œâ”€â”€ hooks/                   # Custom hooks
â””â”€â”€ lib/                     # Utilidades
```

**Variables de Entorno:**
```bash
SERVICIO_CHATBOT_URL=http://localhost:5001
SERVICIO_EDUCACION_URL=http://localhost:5002
SERVICIO_SIMULACION_URL=http://localhost:5003
SERVICIO_RADIOGRAFIAS_URL=http://localhost:5004
```

---

## ğŸš€ Inicio RÃ¡pido

### **Prerrequisitos:**
- Python 3.8+
- Node.js 18+
- Git
- CUDA-capable GPU (recomendado: 4+ GPUs para MedGemma 27B)
- Ray Serve instalado y configurado
- vLLM instalado

### **InstalaciÃ³n Paso a Paso:**

#### **1. Clonar y Configurar**

```bash
# Clonar el repositorio
git clone <repository-url>
cd Pruebas-del-IMSS

# Verificar dependencias
./verify-setup.sh
```

#### **2. Configurar vLLM con Ray Serve**

```bash
# Navegar al directorio vllm
cd /home/administrador/vllm

# Iniciar Ray cluster (si no estÃ¡ iniciado)
ray start --head --port=6379

# Iniciar MedGemma 27B en puerto 8000
serve run serve_medgemma:medgemma_app \
  --host 0.0.0.0 \
  --port 8000

# (Opcional) Iniciar NV-Reason-CXR 3B en puerto 8001
serve run serve_nv_reason:nv_reason_app \
  --host 0.0.0.0 \
  --port 8001
```

#### **3. Configurar Variables de Entorno**

Crear archivo `env.local` en la raÃ­z del proyecto:

```bash
# URLs de servicios backend
SERVICIO_CHATBOT_URL=http://localhost:5001
SERVICIO_EDUCACION_URL=http://localhost:5002
SERVICIO_SIMULACION_URL=http://localhost:5003
SERVICIO_RADIOGRAFIAS_URL=http://localhost:5004

# Endpoint vLLM (prioridad)
VLLM_ENDPOINT=http://localhost:8000/v1/

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ConfiguraciÃ³n de desarrollo
NODE_ENV=development
FLASK_ENV=development
```

#### **4. Iniciar Todos los Servicios**

```bash
# Desde la raÃ­z del proyecto
./start-all.sh
```

Este script:
- Verifica dependencias
- Limpia procesos anteriores
- Inicia todos los servicios backend
- Inicia el gateway Next.js
- Muestra estado y URLs de acceso

#### **5. Acceder a la Plataforma**

- **Local**: http://localhost:3000 o http://localhost:3001
- **Red Local**: http://[IP_LOCAL]:3000

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### **Variables de Entorno Completas**

#### **Backend (Chatbot)**
```bash
# Endpoint vLLM (prioridad)
VLLM_ENDPOINT=http://localhost:8000/v1/

# Fallback endpoints (legacy)
OLLAMA_ENDPOINT=http://localhost:11434/v1/
LM_STUDIO_URL=http://localhost:1234

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Base de datos
DATABASE_PATH=chatbot.db

# Seguridad
ENABLE_RATE_LIMITING=true
RATE_LIMIT_PER_MINUTE=60
```

#### **vLLM Ray Serve**
```bash
# ConfiguraciÃ³n de Ray
RAY_ADDRESS=localhost:6379

# ConfiguraciÃ³n de vLLM
CUDA_VISIBLE_DEVICES=0,1,2,3  # GPUs para MedGemma
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### **OptimizaciÃ³n de Rendimiento**

#### **vLLM Configuration**
```python
# En serve_medgemma.py
engine_args = AsyncEngineArgs(
    model="google/medgemma-27b-it",
    tensor_parallel_size=4,           # Ajustar segÃºn GPUs disponibles
    dtype="bfloat16",                 # Usar float16 si hay problemas de VRAM
    max_model_len=8192,               # Reducir si hay OOM errors
    gpu_memory_utilization=0.90,      # Ajustar segÃºn VRAM disponible
    enable_lora=False,
    enforce_eager=True,               # MÃ¡s estable, menos rÃ¡pido
    trust_remote_code=True,
)
```

#### **Autoscaling Configuration**
```python
# Ajustar segÃºn carga esperada
autoscaling_config={
    "min_replicas": 2,                # MÃ­nimo para respuesta rÃ¡pida
    "max_replicas": 16,               # MÃ¡ximo segÃºn recursos
    "target_ongoing_requests": 5      # Umbral de escalado
}
```

### **Monitoreo y Logs**

#### **Ver Logs de Servicios**
```bash
# Logs en tiempo real
tail -f logs/chatbot.log
tail -f logs/educacion.log
tail -f logs/simulacion.log
tail -f logs/radiografias.log
tail -f logs/nv-reason-cxr.log
tail -f logs/gateway.log
```

#### **Ver Estado de Ray Serve**
```bash
# Estado del cluster
ray status

# Estado de deployments
serve status

# Logs de Ray Serve
tail -f /tmp/ray/session_latest/logs/serve/MedGemmaDeployment.log
tail -f /tmp/ray/session_latest/logs/serve/NVReasonCXRDeployment.log
```

#### **Monitoreo de GPU**
```bash
# Uso de GPU en tiempo real
watch -n 1 nvidia-smi

# Procesos vLLM
nvidia-smi | grep vLLM
```

### **Comandos Ãštiles**

```bash
# Ver estado de servicios
./show-network-info.sh

# Detener todos los servicios
./stop-all.sh

# Reiniciar servicios
./start-all.sh restart

# Verificar configuraciÃ³n
./verify-setup.sh
```

---

## ğŸ“Š Arquitectura de Datos

### **Flujo de Datos Completo**

#### **1. Consulta de Chat MÃ©dico**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Usuario â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ HTTP POST /api/chat
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gateway Next.js â”‚ (Puerto 3000)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Proxy Request
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chatbot FastAPI  â”‚ (Puerto 5001)
â”‚ - main.py        â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â–º LangChain System
     â”‚   â””â”€â–º FallbackLLM
     â”‚       â””â”€â–º HTTP Request
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ray Serve        â”‚ (Puerto 8000)
â”‚ - MedGemma 27B   â”‚
â”‚ - 4 GPUs         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â–º Streaming Response (SSE)
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQLite DB        â”‚
â”‚ - Historial      â”‚
â”‚ - Mensajes       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **2. AnÃ¡lisis de Imagen MÃ©dica**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Usuario â”‚ (Sube imagen)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chatbot API      â”‚
â”‚ - medical_analysis.py
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â–º Procesar imagen (base64)
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vLLM MedGemma    â”‚
â”‚ - Multimodal     â”‚
â”‚ - Procesa imagen â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AnÃ¡lisis MÃ©dico  â”‚
â”‚ - Respuesta      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Almacenamiento de Datos**

#### **SQLite (Chatbot)**
- **UbicaciÃ³n**: `chatbot/chatbot.db`
- **Tablas**:
  - `messages`: Historial de mensajes
  - `sessions`: Sesiones de conversaciÃ³n
  - `users`: Usuarios (si hay autenticaciÃ³n)

#### **ChromaDB (RAG)**
- **UbicaciÃ³n**: `data/` (en servicios que usan RAG)
- **Uso**: Vector store para bÃºsqueda semÃ¡ntica
- **Embeddings**: Documentos mÃ©dicos, guÃ­as clÃ­nicas

#### **File System**
- **UbicaciÃ³n**: `chatbot/media/`
- **Estructura**:
  - `image/`: ImÃ¡genes mÃ©dicas
  - `audio/`: Archivos de audio
  - `document/`: PDFs y documentos
  - `compressed/`: Archivos comprimidos

---

## ğŸ§  Modelos de IA Utilizados

### **Google MedGemma 27B**

**Especificaciones:**
- **Tipo**: Modelo multimodal (texto + imÃ¡genes)
- **ParÃ¡metros**: 27 mil millones
- **EspecializaciÃ³n**: Medicina general
- **Formato**: Instruct (IT) - optimizado para instrucciones
- **Licencia**: Health AI Developer Foundations

**ConfiguraciÃ³n vLLM:**
- **Tensor Parallelism**: 4 GPUs
- **PrecisiÃ³n**: bfloat16
- **Contexto MÃ¡ximo**: 8192 tokens
- **VRAM Requerida**: ~20.7GB por GPU Ã— 4 = ~83GB total

**Uso en el Proyecto:**
- Chatbot mÃ©dico (puerto 5001)
- EducaciÃ³n radiolÃ³gica (puerto 5002)
- SimulaciÃ³n de entrevistas (puerto 5003)
- AnÃ¡lisis de imÃ¡genes mÃ©dicas

**Endpoint**: `http://localhost:8000/v1/chat/completions`

### **NVIDIA NV-Reason-CXR 3B**

**Especificaciones:**
- **Tipo**: Modelo de lenguaje visual (VLM)
- **ParÃ¡metros**: 3 mil millones
- **EspecializaciÃ³n**: RadiografÃ­as de tÃ³rax
- **CaracterÃ­stica**: Cadena de razonamiento paso a paso
- **Licencia**: NVIDIA OneWay Non-Commercial

**ConfiguraciÃ³n vLLM:**
- **Tensor Parallelism**: 2 GPUs
- **PrecisiÃ³n**: bfloat16
- **Contexto MÃ¡ximo**: 8192 tokens
- **VRAM Requerida**: ~10GB por GPU Ã— 2 = ~20GB total

**Uso en el Proyecto:**
- AnÃ¡lisis especializado de radiografÃ­as (puerto 5005)
- RadiografÃ­as de tÃ³rax con RAG (puerto 5004)

**Endpoint**: `http://localhost:8001/v1/chat/completions`

### **LangChain Framework**

**Rol**: Orquestador y framework de integraciÃ³n

**Componentes Utilizados:**
- **ChatOpenAI**: Cliente para API compatible OpenAI (vLLM)
- **ChatMessageHistory**: GestiÃ³n de memoria de conversaciÃ³n
- **RunnableSequence**: Cadenas de procesamiento
- **OutputParsers**: Parsing de respuestas estructuradas
- **Prompts**: Templates especializados mÃ©dicos

**IntegraciÃ³n:**
- Conecta servicios backend con modelos vLLM
- Maneja streaming, fallbacks, y optimizaciones
- Proporciona abstracciÃ³n sobre diferentes proveedores

---

## ğŸ”„ Flujos de Trabajo Detallados

### **Flujo 1: Consulta MÃ©dica en Chatbot**

1. **Usuario envÃ­a mensaje** â†’ Gateway Next.js
2. **Gateway** â†’ Proxy a Chatbot API (FastAPI:5001)
3. **Chatbot API** â†’ Valida request, autentica usuario
4. **LangChain System** â†’ Procesa mensaje con contexto histÃ³rico
5. **FallbackLLM** â†’ Conecta a vLLM Ray Serve (puerto 8000)
6. **Ray Serve** â†’ Distribuye a rÃ©plica disponible de MedGemma 27B
7. **MedGemma 27B** â†’ Genera respuesta mÃ©dica
8. **Streaming Response** â†’ SSE al frontend
9. **Frontend** â†’ Muestra respuesta en tiempo real
10. **SQLite** â†’ Persiste conversaciÃ³n

### **Flujo 2: AnÃ¡lisis de RadiografÃ­a**

1. **Usuario sube imagen** â†’ Gateway Next.js
2. **Gateway** â†’ Proxy a servicio correspondiente
3. **Servicio** â†’ Convierte imagen a base64
4. **vLLM Endpoint** â†’ Request multimodal con imagen
5. **MedGemma/NV-Reason** â†’ Procesa imagen + prompt
6. **AnÃ¡lisis Generado** â†’ Respuesta estructurada
7. **Frontend** â†’ Muestra anÃ¡lisis con visualizaciÃ³n

### **Flujo 3: BÃºsqueda RAG (RadiografÃ­as)**

1. **Usuario pregunta** â†’ Servicio RadiografÃ­as
2. **RAG System** â†’ Busca en ChromaDB
3. **ChromaDB** â†’ Retorna documentos relevantes
4. **Contexto Enriquecido** â†’ Prompt + documentos
5. **vLLM** â†’ Genera respuesta con contexto
6. **Respuesta** â†’ Incluye referencias a documentos

---

## ğŸ› ï¸ Desarrollo y ContribuciÃ³n

### **Estructura del Proyecto**

```
Pruebas-del-IMSS/
â”œâ”€â”€ chatbot/                    # Chatbot mÃ©dico (FastAPI)
â”‚   â”œâ”€â”€ main.py                 # App principal FastAPI
â”‚   â”œâ”€â”€ langchain_system.py    # Sistema LangChain
â”‚   â”œâ”€â”€ memory_manager.py      # GestiÃ³n de memoria
â”‚   â”œâ”€â”€ medical_analysis.py    # AnÃ¡lisis de imÃ¡genes
â”‚   â”œâ”€â”€ prompts/               # Prompts mÃ©dicos
â”‚   â””â”€â”€ requirements.txt       # Dependencias Python
â”‚
â”œâ”€â”€ Educacion_radiografia/     # EducaciÃ³n radiolÃ³gica (Flask)
â”‚   â”œâ”€â”€ app.py                 # App Flask
â”‚   â”œâ”€â”€ local_llm_client.py    # Cliente MedGemma
â”‚   â””â”€â”€ routes.py              # Rutas API
â”‚
â”œâ”€â”€ Simulacion/                # SimulaciÃ³n entrevistas (Flask)
â”‚   â”œâ”€â”€ app.py                 # App Flask
â”‚   â”œâ”€â”€ interview_simulator.py # Simulador
â”‚   â””â”€â”€ frontend/              # React frontend
â”‚
â”œâ”€â”€ radiografias_torax/        # RadiografÃ­as con RAG
â”‚   â”œâ”€â”€ backend/               # Flask backend
â”‚   â”œâ”€â”€ frontend/              # React frontend
â”‚   â””â”€â”€ data/                  # ChromaDB data
â”‚
â”œâ”€â”€ nv-reason-cxr/             # NV-Reason-CXR (Gradio)
â”‚   â”œâ”€â”€ app.py                 # App Gradio
â”‚   â””â”€â”€ run_local.sh           # Script inicio
â”‚
â”œâ”€â”€ UI_IMSS/                   # Gateway Next.js
â”‚   â”œâ”€â”€ app/                   # App Router
â”‚   â”œâ”€â”€ components/            # Componentes React
â”‚   â””â”€â”€ package.json          # Dependencias Node
â”‚
â”œâ”€â”€ vllm/                      # Scripts vLLM (referencia)
â”‚   â”œâ”€â”€ serve_medgemma.py     # Deployment MedGemma
â”‚   â””â”€â”€ serve_nv_reason.py    # Deployment NV-Reason
â”‚
â”œâ”€â”€ logs/                      # Logs de servicios
â”œâ”€â”€ scripts/                   # Scripts de utilidad
â”œâ”€â”€ k8s/                       # ConfiguraciÃ³n Kubernetes
â”‚
â”œâ”€â”€ start-all.sh               # Script inicio unificado
â”œâ”€â”€ stop-all.sh               # Script detener servicios
â”œâ”€â”€ verify-setup.sh           # Verificar configuraciÃ³n
â””â”€â”€ README.md                  # Este archivo
```

### **GuÃ­a de ContribuciÃ³n**

1. **Fork del repositorio**
2. **Crear rama feature**: `git checkout -b feature/nueva-funcionalidad`
3. **Desarrollar cambios**:
   - Seguir convenciones de cÃ³digo existentes
   - Agregar tests si es posible
   - Actualizar documentaciÃ³n
4. **Commit cambios**: `git commit -m 'Agregar nueva funcionalidad'`
5. **Push a la rama**: `git push origin feature/nueva-funcionalidad`
6. **Crear Pull Request**

### **Convenciones de CÃ³digo**

- **Python**: PEP 8, type hints cuando sea posible
- **TypeScript**: ESLint configurado, strict mode
- **Commits**: Mensajes descriptivos en espaÃ±ol
- **DocumentaciÃ³n**: Actualizar READMEs relevantes

---

## ğŸ“Š Monitoreo y Troubleshooting

### **VerificaciÃ³n de Estado**

```bash
# Estado de servicios
./show-network-info.sh

# Verificar puertos
netstat -tulpn | grep -E ':(3000|5001|5002|5003|5004|5005|8000|8001)'

# Estado de Ray Serve
ray status
serve status
```

### **Problemas Comunes y Soluciones**

#### **1. vLLM no responde**

**SÃ­ntomas**: Timeout o error 500 en requests

**Soluciones**:
```bash
# Verificar que Ray Serve estÃ¡ corriendo
ray status

# Verificar logs de Ray Serve
tail -f /tmp/ray/session_latest/logs/serve/MedGemmaDeployment.log

# Verificar uso de GPU
nvidia-smi

# Reiniciar Ray Serve
serve shutdown
serve run serve_medgemma:medgemma_app --host 0.0.0.0 --port 8000
```

#### **2. Out of Memory (OOM) en GPU**

**SÃ­ntomas**: Error de VRAM insuficiente

**Soluciones**:
- Reducir `gpu_memory_utilization` en `serve_medgemma.py`
- Reducir `max_model_len`
- Usar `dtype="float16"` en lugar de `bfloat16`
- Reducir `tensor_parallel_size` si es posible

#### **3. Servicios no inician**

**SÃ­ntomas**: Puertos ocupados o errores de inicio

**Soluciones**:
```bash
# Limpiar procesos anteriores
./stop-all.sh

# Verificar puertos ocupados
lsof -i :5001  # Reemplazar con puerto problemÃ¡tico

# Matar proceso si es necesario
kill -9 <PID>
```

#### **4. CORS Errors**

**SÃ­ntomas**: Errores de CORS en navegador

**Soluciones**:
- Verificar `CORS_ORIGINS` en variables de entorno
- Asegurar que gateway y servicios usan las mismas URLs
- Verificar que middleware CORS estÃ¡ configurado correctamente

---

## ğŸ“„ Licencias y Atribuciones

### **Modelos de IA:**
- **MedGemma 27B**: Health AI Developer Foundations License
- **NV-Reason-CXR 3B**: NVIDIA OneWay Non-Commercial License
- **LangChain**: MIT License

### **TecnologÃ­as:**
- **Next.js**: MIT License
- **React**: MIT License
- **Flask**: BSD License
- **FastAPI**: MIT License
- **Gradio**: Apache 2.0 License
- **vLLM**: Apache 2.0 License
- **Ray Serve**: Apache 2.0 License

### **Agradecimientos:**
- **Google Health** por MedGemma
- **NVIDIA** por NV-Reason-CXR y vLLM
- **LangChain** por el framework de orquestaciÃ³n
- **Ray Project** por Ray Serve
- **IMSS** por la oportunidad de desarrollo
- **Comunidad Open Source** por las herramientas utilizadas

---

## âš ï¸ Descargo de Responsabilidad

**IMPORTANTE**: Esta plataforma es Ãºnicamente para fines educativos y de investigaciÃ³n. No debe ser utilizada para diagnÃ³stico clÃ­nico real sin supervisiÃ³n mÃ©dica profesional. Todos los anÃ¡lisis y recomendaciones generados por la IA deben ser verificados por profesionales de la salud calificados.

**Limitaciones:**
- Los modelos de IA pueden cometer errores
- No reemplaza el juicio clÃ­nico profesional
- Siempre consultar con mÃ©dicos certificados para diagnÃ³sticos
- Los resultados son sugerencias educativas, no diagnÃ³sticos definitivos

---

## ğŸ“ Soporte y Recursos

### **DocumentaciÃ³n Adicional:**
- Ver READMEs individuales en cada directorio de servicio
- DocumentaciÃ³n de anÃ¡lisis en archivos `ANALISIS_*.md`
- GuÃ­as de optimizaciÃ³n en `ESTRATEGIAS_*.md`

### **Recursos TÃ©cnicos:**
- **vLLM Documentation**: https://docs.vllm.ai/
- **Ray Serve Documentation**: https://docs.ray.io/en/latest/serve/
- **LangChain Documentation**: https://python.langchain.com/
- **MedGemma**: https://huggingface.co/google/medgemma-27b-it
- **NV-Reason-CXR**: https://huggingface.co/nvidia/NV-Reason-CXR-3B

---

## ğŸ‰ ConclusiÃ³n

La **Suite IMSS** representa una implementaciÃ³n completa de una plataforma de IA mÃ©dica utilizando tecnologÃ­as de vanguardia:

- **Inferencia distribuida** con vLLM y Ray Serve
- **Autoscaling inteligente** para manejar carga variable
- **IntegraciÃ³n multimodal** para anÃ¡lisis de imÃ¡genes
- **Arquitectura modular** con servicios independientes
- **Streaming en tiempo real** para mejor UX
- **RAG avanzado** para conocimiento contextualizado

Esta plataforma demuestra el potencial de la IA especializada en medicina para mejorar la educaciÃ³n mÃ©dica y el apoyo clÃ­nico, siempre con la supervisiÃ³n y validaciÃ³n de profesionales de la salud.

---

