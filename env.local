# Configuración para pruebas locales
# Copia este archivo como .env.local

# Puertos de servicios locales
FLASK_PORT_CHATBOT=5001
FLASK_PORT_EDUCACION=5002
FLASK_PORT_SIMULACION=5003
FLASK_PORT_RADIOGRAFIAS=5004
NEXT_PORT=3000

# URLs de servicios locales
SERVICIO_CHATBOT_URL=http://localhost:5001
SERVICIO_EDUCACION_URL=http://localhost:5002
SERVICIO_SIMULACION_URL=http://localhost:5003
SERVICIO_RADIOGRAFIAS_URL=http://localhost:5004

# Configuración de desarrollo
NODE_ENV=development
FLASK_ENV=development

# Configuración de MedGemma - vLLM con Ray Serve (prioridad)
VLLM_ENDPOINT=http://localhost:8000/v1/
# Configuración de MedGemma - Ollama (fallback/legacy)
MEDGEMMA_ENDPOINT=
OLLAMA_ENDPOINT=http://localhost:11434/v1/
# Compatibilidad con LM_STUDIO_URL (si no está OLLAMA_ENDPOINT)
LM_STUDIO_URL=http://localhost:11434

# Configuración de Gemini (opcional para pruebas)
GEMINI_API_KEY=
GCP_MEDGEMMA_ENDPOINT=
GCP_MEDGEMMA_SERVICE_ACCOUNT_KEY=

# Configuración de logging
LOG_LEVEL=INFO
