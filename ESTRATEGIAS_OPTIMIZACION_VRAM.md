# Estrategias de Optimizaci√≥n de VRAM - An√°lisis Detallado

**Fecha:** 8 de Noviembre 2025  
**Contexto:** 16 nodos disponibles, cada uno con 4 GPUs (NVIDIA A10, 23 GB cada una)

## üìä Situaci√≥n Actual

- **Nodo 0:** 4 GPUs con modelo vLLM (medgemma-27b) corriendo
  - Consumo actual: ~82.9 GB por nodo
  - `gpu_memory_utilization=0.95` (95% de VRAM)
  - `tensor_parallel_size=4` (1 r√©plica usa 4 GPUs)
- **Nodos 1-15:** Disponibles para configurar
- **Objetivo:** Montar medgemma-4b en el nodo 0 o encontrar equilibrio

---

## üéØ Estrategia 1: Reducir `gpu_memory_utilization` a 0.90 o 0.93 en Nodo 0

### An√°lisis de Viabilidad

#### Opci√≥n A: Reducir a 0.90

**C√°lculo de VRAM liberada:**
- GPU Total: 23 GB (23,552 MiB)
- Actual: `gpu_memory_utilization=0.95` ‚Üí 21.85 GB por GPU (22,374 MiB)
- Propuesto: `gpu_memory_utilization=0.90` ‚Üí 20.70 GB por GPU (21,197 MiB)
- **VRAM liberada por GPU:** ~1.15 GB (1,178 MiB)
- **Total liberado en 4 GPUs:** ~4.60 GB
- **VRAM libre total con 0.90:** ~9.20 GB (en 4 GPUs)
- **Necesario para medgemma-4b:** ~5.0 GB

**C√°lculo detallado:**
- Consumo con 0.90 en 4 GPUs: ~82.80 GB
- VRAM total en 4 GPUs: ~92.00 GB
- VRAM libre: ~9.20 GB
- medgemma-4b necesita: ~5.0 GB
- **Margen disponible:** ~4.20 GB ‚úÖ

#### Opci√≥n B: Reducir a 0.93

**C√°lculo de VRAM liberada:**
- GPU Total: 23 GB (23,552 MiB)
- Actual: `gpu_memory_utilization=0.95` ‚Üí 21.85 GB por GPU (22,374 MiB)
- Propuesto: `gpu_memory_utilization=0.93` ‚Üí 21.39 GB por GPU (21,903 MiB)
- **VRAM liberada por GPU:** ~0.46 GB (471 MiB)
- **Total liberado en 4 GPUs:** ~1.84 GB
- **VRAM libre total con 0.93:** ~6.44 GB (en 4 GPUs)
- **Necesario para medgemma-4b:** ~5.0 GB

**C√°lculo detallado:**
- Consumo con 0.93 en 4 GPUs: ~85.56 GB
- VRAM total en 4 GPUs: ~92.00 GB
- VRAM libre: ~6.44 GB
- medgemma-4b necesita: ~5.0 GB
- **Margen disponible:** ~1.44 GB ‚ö†Ô∏è

### ‚úÖ Viabilidad: **POSIBLE y VIABLE (con diferencias)**

| Configuraci√≥n | VRAM Libre | Margen para medgemma-4b | Recomendaci√≥n |
|---------------|------------|-------------------------|---------------|
| **0.90** | ~9.20 GB | ~4.20 GB | ‚úÖ Recomendado (m√°s seguro) |
| **0.93** | ~6.44 GB | ~1.44 GB | ‚ö†Ô∏è Ajustado (funciona pero con poco margen) |

**Ventajas:**
- Cambio simple (solo modificar un par√°metro)
- No requiere reconfiguraci√≥n del cluster
- Mantiene el modelo 27B corriendo
- **0.90:** Mayor margen de seguridad (~4.2 GB)
- **0.93:** Menor impacto en rendimiento del modelo 27B

**Desventajas:**
- Puede afectar el rendimiento del modelo 27B (menos KV cache)
- **0.90:** Impacto moderado en rendimiento
- **0.93:** Impacto m√≠nimo en rendimiento, pero margen muy ajustado (~1.44 GB)

**Implementaci√≥n:**

**Opci√≥n A (0.90 - Recomendada):**
```python
# En serve_medgemma.py, l√≠nea 84
gpu_memory_utilization=0.90,  # Reducir de 0.95 a 0.90
```

**Opci√≥n B (0.93 - M√°s conservadora):**
```python
# En serve_medgemma.py, l√≠nea 84
gpu_memory_utilization=0.93,  # Reducir de 0.95 a 0.93
```

**Recomendaci√≥n:**
- **0.90:** ‚úÖ **VIABLE y RECOMENDADO** - Con margen de ~4.2 GB, es suficiente para medgemma-4b. La reducci√≥n de KV cache puede afectar ligeramente el rendimiento, pero es aceptable.
- **0.93:** ‚ö†Ô∏è **VIABLE pero AJUSTADO** - Con margen de solo ~1.44 GB, funciona pero es muy ajustado. Mejor rendimiento del modelo 27B, pero mayor riesgo de OOM si hay picos de uso.

---

## üéØ Estrategia 2: Excluir Nodo 0 de vLLM, Solo Correr medgemma-27b en Nodo 0

### An√°lisis de Viabilidad

**Concepto:**
- Nodo 0: Solo medgemma-27b (sin vLLM para otros modelos)
- Nodos 1-15: vLLM con medgemma-27b (o medgemma-4b)

**Configuraci√≥n necesaria:**
1. **Crear dos deployments separados:**
   - `MedGemma27BDeployment`: Para nodos 1-15 (con autoscaling)
   - `MedGemma27BNode0Deployment`: Solo para nodo 0 (sin autoscaling)

2. **Usar Ray Placement Groups para excluir nodo 0:**
   - Ray Serve permite especificar en qu√© nodos correr deployments
   - Usar `ray.util.placement_group` o `ray_actor_options` con `resources`

### ‚úÖ Viabilidad: **POSIBLE, pero complejo**

**Ventajas:**
- Nodo 0 dedicado exclusivamente a medgemma-27b
- Nodos 1-15 pueden correr vLLM con autoscaling
- Separaci√≥n clara de recursos

**Desventajas:**
- Requiere modificar la arquitectura de Ray Serve
- Necesita dos deployments separados
- Configuraci√≥n m√°s compleja

**Implementaci√≥n detallada:**

#### Opci√≥n A: Usar Placement Groups (Recomendado)

```python
# serve_medgemma.py - Modificar para soportar exclusi√≥n de nodo 0

from ray.util.placement_group import placement_group, remove_placement_group
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

# Crear placement group que excluye nodo 0
# Esto requiere identificar nodo 0 por su IP o hostname
NODE_0_HOSTNAME = os.environ.get("NODE_0_HOSTNAME", "node-0")

# Configuraci√≥n para nodos 1-15 (excluye nodo 0)
@serve.deployment(
    name="MedGemma27BDeployment",
    autoscaling_config={
        "min_replicas": 1,
        "max_replicas": 15,  # Solo 15 nodos (excluye nodo 0)
        "target_ongoing_requests": 5
    },
    ray_actor_options={
        "num_gpus": 4,
        "resources": {"node_id": "!node_0"}  # Excluir nodo 0
    }
)
class VLLMDeploymentNodes1_15:
    # ... mismo c√≥digo que VLLMDeployment ...
    pass

# Configuraci√≥n para nodo 0 (solo medgemma-27b, sin autoscaling)
@serve.deployment(
    name="MedGemma27BNode0Deployment",
    num_replicas=1,  # Solo 1 r√©plica en nodo 0
    ray_actor_options={
        "num_gpus": 4,
        "resources": {"node_id": "node_0"}  # Solo nodo 0
    }
)
class VLLMDeploymentNode0:
    # ... mismo c√≥digo que VLLMDeployment ...
    pass
```

#### Opci√≥n B: Usar Ray Cluster Labels (M√°s simple)

```python
# En el script de inicio de Ray, etiquetar nodo 0
# ray start --head --labels={"node_type":"dedicated_27b"}

@serve.deployment(
    name="MedGemma27BDeployment",
    autoscaling_config={
        "min_replicas": 1,
        "max_replicas": 15,
        "target_ongoing_requests": 5
    },
    ray_actor_options={
        "num_gpus": 4,
        "resources": {"node_type": "!dedicated_27b"}  # Excluir nodo 0
    }
)
```

**Recomendaci√≥n:** ‚úÖ **Factible** - Requiere configuraci√≥n de Ray cluster, pero es la soluci√≥n m√°s limpia.

---

## üéØ Estrategia 3: Equilibrio entre vLLM y medgemma-4b en Nodo 0

### An√°lisis de Viabilidad

**Concepto:**
- Nodo 0: medgemma-27b (vLLM) + medgemma-4b (Ollama o vLLM separado)
- Nodos 1-15: vLLM con medgemma-27b (autoscaling)

**C√°lculo de VRAM necesario:**
- medgemma-27b (vLLM): ~82.9 GB (4 GPUs)
- medgemma-4b (Ollama): ~5.0 GB (1 GPU)
- **Total necesario:** ~87.9 GB en 4 GPUs

**Problema:**
- Cada GPU tiene 23 GB
- 4 GPUs = 92 GB total
- medgemma-27b usa ~20.7 GB por GPU = ~82.8 GB
- medgemma-4b necesita ~5.0 GB
- **Total:** ~87.8 GB (muy ajustado, solo ~4.2 GB libres)

### ‚úÖ Viabilidad: **POSIBLE, pero muy ajustado**

**Opciones de implementaci√≥n:**

#### Opci√≥n A: medgemma-27b con `tensor_parallel_size=3` + medgemma-4b en 1 GPU

**C√°lculo:**
- medgemma-27b en 3 GPUs: ~20.7 GB √ó 3 = ~62.1 GB
- medgemma-4b en 1 GPU: ~5.0 GB
- **Total:** ~67.1 GB (margen: ~24.9 GB)

**Implementaci√≥n:**
```python
# Deployment 1: medgemma-27b en 3 GPUs (nodo 0)
@serve.deployment(
    name="MedGemma27BNode0",
    num_replicas=1,
    ray_actor_options={
        "num_gpus": 3,  # Solo 3 GPUs
        "resources": {"node_id": "node_0"}
    }
)
class VLLMDeployment27B:
    def __init__(self):
        engine_args = AsyncEngineArgs(
            model="google/medgemma-27b-it",
            tensor_parallel_size=3,  # 3 GPUs en lugar de 4
            gpu_memory_utilization=0.90,
            # ... resto de configuraci√≥n
        )
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)

# Deployment 2: medgemma-4b en 1 GPU (nodo 0)
@serve.deployment(
    name="MedGemma4BNode0",
    num_replicas=1,
    ray_actor_options={
        "num_gpus": 1,  # Solo 1 GPU
        "resources": {"node_id": "node_0"}
    }
)
class VLLMDeployment4B:
    def __init__(self):
        engine_args = AsyncEngineArgs(
            model="amsaravi/medgemma-4b-it:q8",
            tensor_parallel_size=1,  # 1 GPU
            gpu_memory_utilization=0.85,
            # ... resto de configuraci√≥n
        )
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)
```

**Ventajas:**
- Ambos modelos en nodo 0
- Margen de seguridad adecuado
- medgemma-27b sigue funcionando (aunque m√°s lento con 3 GPUs)

**Desventajas:**
- medgemma-27b m√°s lento (3 GPUs vs 4 GPUs)
- Configuraci√≥n m√°s compleja

#### Opci√≥n B: medgemma-27b con `gpu_memory_utilization=0.85` + medgemma-4b en Ollama

**C√°lculo:**
- medgemma-27b: ~20.7 GB √ó 4 GPUs √ó 0.85/0.95 = ~18.5 GB √ó 4 = ~74 GB
- medgemma-4b (Ollama): ~5.0 GB (puede correr en CPU o 1 GPU)
- **Total:** ~79 GB (margen: ~13 GB)

**Implementaci√≥n:**
```python
# Deployment 1: medgemma-27b con menos memoria
@serve.deployment(
    name="MedGemma27BNode0",
    num_replicas=1,
    ray_actor_options={
        "num_gpus": 4,
        "resources": {"node_id": "node_0"}
    }
)
class VLLMDeployment27B:
    def __init__(self):
        engine_args = AsyncEngineArgs(
            model="google/medgemma-27b-it",
            tensor_parallel_size=4,
            gpu_memory_utilization=0.85,  # Reducir a 85%
            # ... resto de configuraci√≥n
        )
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)

# Deployment 2: medgemma-4b con Ollama (separado, no vLLM)
# Esto requiere correr Ollama como servicio separado en el nodo 0
```

**Ventajas:**
- medgemma-27b mantiene 4 GPUs (mejor rendimiento)
- Ollama es m√°s ligero que vLLM para modelos peque√±os

**Desventajas:**
- Requiere Ollama corriendo en paralelo
- Dos sistemas diferentes (vLLM + Ollama)

**Recomendaci√≥n:** ‚úÖ **Factible** - Opci√≥n A es m√°s limpia (todo en vLLM), Opci√≥n B es m√°s simple (Ollama separado).

---

## üìã Resumen de Estrategias

| Estrategia | Viabilidad | Complejidad | Rendimiento | Recomendaci√≥n |
|------------|------------|-------------|-------------|---------------|
| **1A. Reducir a 0.90** | ‚úÖ Viable | üü¢ Baja | üü° Medio | ‚úÖ Recomendado (m√°s seguro) |
| **1B. Reducir a 0.93** | ‚ö†Ô∏è Ajustado | üü¢ Baja | üü¢ Alto | ‚ö†Ô∏è Funciona pero con poco margen |
| **2. Excluir Nodo 0** | ‚úÖ Factible | üü° Media | üü¢ Alto | ‚úÖ Recomendado |
| **3A. 27B en 3 GPUs + 4B en 1 GPU** | ‚úÖ Factible | üü° Media | üü° Medio | ‚úÖ Recomendado |
| **3B. 27B 0.85 + 4B Ollama** | ‚úÖ Factible | üü¢ Baja | üü¢ Alto | ‚úÖ Recomendado |

---

## üöÄ Recomendaci√≥n Final

### Opci√≥n Recomendada: **Estrategia 1A** (Reducir a 0.90) - **M√ÅS SIMPLE Y SEGURA**

**Razones:**
1. ‚úÖ **M√°s simple:** Solo cambiar un par√°metro
2. ‚úÖ **Suficiente margen:** ~4.2 GB libres (m√°s que suficiente para medgemma-4b)
3. ‚úÖ **No requiere reconfiguraci√≥n:** Mantiene la arquitectura actual
4. ‚úÖ **R√°pido de implementar:** Cambio m√≠nimo en c√≥digo
5. ‚úÖ **Seguro:** Margen adecuado para evitar OOM

### Alternativa: **Estrategia 1B** (Reducir a 0.93) - **M√ÅS CONSERVADORA**

**Razones:**
1. ‚úÖ **M√°s simple:** Solo cambiar un par√°metro
2. ‚ö†Ô∏è **Margen ajustado:** ~1.44 GB libres (justo suficiente para medgemma-4b)
3. ‚úÖ **No requiere reconfiguraci√≥n:** Mantiene la arquitectura actual
4. ‚úÖ **Mejor rendimiento:** Menor impacto en el modelo 27B (m√°s KV cache)
5. ‚ö†Ô∏è **Riesgo:** Margen muy ajustado, mayor riesgo de OOM en picos de uso

**Recomendaci√≥n entre 0.90 y 0.93:**
- **Usar 0.90** si priorizas seguridad y margen de error
- **Usar 0.93** si priorizas rendimiento del modelo 27B y est√°s dispuesto a aceptar el riesgo de margen ajustado

### Alternativa Recomendada: **Estrategia 3A** (27B en 3 GPUs + 4B en 1 GPU) - **M√ÅS SEGURA**

**Razones:**
1. ‚úÖ **Mayor margen:** ~24.9 GB libres (m√°s seguro)
2. ‚úÖ **Mejor rendimiento:** medgemma-27b mantiene buen rendimiento en 3 GPUs
3. ‚úÖ **Todo en vLLM:** Consistencia en la arquitectura
4. ‚úÖ **Separaci√≥n clara:** Cada modelo en sus GPUs dedicadas

### Implementaci√≥n Detallada por Estrategia:

#### Estrategia 1: Reducir a 0.90 o 0.93 (M√ÅS SIMPLE)

**Opci√≥n A: Reducir a 0.90 (RECOMENDADA)**

**Pasos:**
1. Modificar `serve_medgemma.py`, l√≠nea 84:
   ```python
   gpu_memory_utilization=0.90,  # Cambiar de 0.95 a 0.90
   ```

2. Reiniciar el servicio vLLM

3. Montar medgemma-4b en Ollama o vLLM separado en el nodo 0

**C√≥digo exacto:**
```python
# vllm/serve_medgemma.py - L√≠nea 79-89
engine_args = AsyncEngineArgs(
    model=MODEL_PATH,
    tensor_parallel_size=4,
    dtype="bfloat16",
    max_model_len=8192,
    gpu_memory_utilization=0.90,  # ‚Üê CAMBIO AQU√ç (de 0.95 a 0.90)
    enable_lora=False,
    enforce_eager=True,
    trust_remote_code=True,
    download_dir=MODELS_BASE_DIR,
)
```

**Ventajas:**
- ‚úÖ Cambio m√≠nimo (1 l√≠nea)
- ‚úÖ No requiere reconfiguraci√≥n de Ray
- ‚úÖ Mantiene arquitectura actual
- ‚úÖ Margen de seguridad adecuado (~4.2 GB)

**Desventajas:**
- ‚ö†Ô∏è Puede afectar rendimiento del modelo 27B (menos KV cache)

---

**Opci√≥n B: Reducir a 0.93 (M√ÅS CONSERVADORA)**

**Pasos:**
1. Modificar `serve_medgemma.py`, l√≠nea 84:
   ```python
   gpu_memory_utilization=0.93,  # Cambiar de 0.95 a 0.93
   ```

2. Reiniciar el servicio vLLM

3. Montar medgemma-4b en Ollama o vLLM separado en el nodo 0

**C√≥digo exacto:**
```python
# vllm/serve_medgemma.py - L√≠nea 79-89
engine_args = AsyncEngineArgs(
    model=MODEL_PATH,
    tensor_parallel_size=4,
    dtype="bfloat16",
    max_model_len=8192,
    gpu_memory_utilization=0.93,  # ‚Üê CAMBIO AQU√ç (de 0.95 a 0.93)
    enable_lora=False,
    enforce_eager=True,
    trust_remote_code=True,
    download_dir=MODELS_BASE_DIR,
)
```

**Ventajas:**
- ‚úÖ Cambio m√≠nimo (1 l√≠nea)
- ‚úÖ No requiere reconfiguraci√≥n de Ray
- ‚úÖ Mantiene arquitectura actual
- ‚úÖ Menor impacto en rendimiento del modelo 27B (m√°s KV cache)

**Desventajas:**
- ‚ö†Ô∏è Margen muy ajustado (~1.44 GB)
- ‚ö†Ô∏è Mayor riesgo de OOM si hay picos de uso

**Comparaci√≥n:**

| Configuraci√≥n | VRAM Libre | Margen | Impacto Rendimiento 27B | Recomendaci√≥n |
|---------------|------------|--------|------------------------|---------------|
| **0.90** | ~9.20 GB | ~4.20 GB | Moderado | ‚úÖ Recomendado |
| **0.93** | ~6.44 GB | ~1.44 GB | M√≠nimo | ‚ö†Ô∏è Ajustado |

---

#### Estrategia 2: Excluir Nodo 0 de vLLM

**Pasos:**
1. **Etiquetar nodo 0 en Ray cluster:**
   ```bash
   # Al iniciar Ray en nodo 0
   ray start --head --labels='{"node_type":"dedicated_27b"}'
   
   # Al iniciar Ray en nodos 1-15
   ray start --address=<head-node-address> --labels='{"node_type":"vllm_worker"}'
   ```

2. **Modificar `serve_medgemma.py` para crear dos deployments:**
   ```python
   # Deployment para nodos 1-15 (excluye nodo 0)
   @serve.deployment(
       name="MedGemma27BDeployment",
       autoscaling_config={
           "min_replicas": 1,
           "max_replicas": 15,  # Solo 15 nodos
           "target_ongoing_requests": 5
       },
       ray_actor_options={
           "num_gpus": 4,
           "resources": {"node_type": "vllm_worker"}  # Solo nodos 1-15
       }
   )
   class VLLMDeploymentNodes1_15:
       # ... c√≥digo actual ...
       pass
   
   # Deployment para nodo 0 (solo medgemma-27b, sin autoscaling)
   @serve.deployment(
       name="MedGemma27BNode0Deployment",
       num_replicas=1,  # Solo 1 r√©plica en nodo 0
       ray_actor_options={
           "num_gpus": 4,
           "resources": {"node_type": "dedicated_27b"}  # Solo nodo 0
       }
   )
   class VLLMDeploymentNode0:
       # ... c√≥digo actual ...
       pass
   ```

3. **Montar medgemma-4b en nodo 0** (Ollama o vLLM separado)

**Ventajas:**
- ‚úÖ Nodo 0 dedicado exclusivamente a medgemma-27b
- ‚úÖ Nodos 1-15 pueden usar autoscaling completo
- ‚úÖ Separaci√≥n clara de recursos

**Desventajas:**
- ‚ö†Ô∏è Requiere reconfiguraci√≥n de Ray cluster
- ‚ö†Ô∏è M√°s complejo de mantener

---

#### Estrategia 3A: 27B en 3 GPUs + 4B en 1 GPU (M√ÅS SEGURA)

**Pasos:**
1. **Modificar `serve_medgemma.py` para crear dos deployments en nodo 0:**
   ```python
   # Deployment 1: medgemma-27b en 3 GPUs (nodo 0)
   @serve.deployment(
       name="MedGemma27BNode0",
       num_replicas=1,
       ray_actor_options={
           "num_gpus": 3,  # Solo 3 GPUs
           "resources": {"node_type": "dedicated_27b"}  # Solo nodo 0
       }
   )
   class VLLMDeployment27B:
       def __init__(self):
           engine_args = AsyncEngineArgs(
               model="google/medgemma-27b-it",
               tensor_parallel_size=3,  # 3 GPUs en lugar de 4
               dtype="bfloat16",
               max_model_len=8192,
               gpu_memory_utilization=0.90,
               enable_lora=False,
               enforce_eager=True,
               trust_remote_code=True,
           )
           self.engine = AsyncLLMEngine.from_engine_args(engine_args)
       # ... resto del c√≥digo ...
   
   # Deployment 2: medgemma-4b en 1 GPU (nodo 0)
   @serve.deployment(
       name="MedGemma4BNode0",
       num_replicas=1,
       ray_actor_options={
           "num_gpus": 1,  # Solo 1 GPU
           "resources": {"node_type": "dedicated_27b"}  # Solo nodo 0
       }
   )
   class VLLMDeployment4B:
       def __init__(self):
           engine_args = AsyncEngineArgs(
               model="amsaravi/medgemma-4b-it:q8",
               tensor_parallel_size=1,  # 1 GPU
               dtype="bfloat16",
               max_model_len=8192,
               gpu_memory_utilization=0.85,
               enable_lora=False,
               enforce_eager=True,
               trust_remote_code=True,
           )
           self.engine = AsyncLLMEngine.from_engine_args(engine_args)
       # ... resto del c√≥digo ...
   ```

2. **Mantener deployment para nodos 1-15:**
   ```python
   @serve.deployment(
       name="MedGemma27BDeployment",
       autoscaling_config={
           "min_replicas": 1,
           "max_replicas": 15,
           "target_ongoing_requests": 5
       },
       ray_actor_options={
           "num_gpus": 4,
           "resources": {"node_type": "vllm_worker"}  # Solo nodos 1-15
       }
   )
   class VLLMDeploymentNodes1_15:
       # ... c√≥digo actual con tensor_parallel_size=4 ...
       pass
   ```

3. **Configurar routing en el frontend/API:**
   - Peticiones a medgemma-4b ‚Üí `MedGemma4BNode0` (nodo 0)
   - Peticiones a medgemma-27b ‚Üí `MedGemma27BDeployment` (nodos 1-15) o `MedGemma27BNode0` (nodo 0)

**Ventajas:**
- ‚úÖ Mayor margen de seguridad (~24.9 GB libres)
- ‚úÖ Ambos modelos en el mismo nodo
- ‚úÖ Todo en vLLM (consistencia)

**Desventajas:**
- ‚ö†Ô∏è medgemma-27b m√°s lento (3 GPUs vs 4 GPUs)
- ‚ö†Ô∏è Configuraci√≥n m√°s compleja

---

#### Estrategia 3B: 27B 0.85 + 4B Ollama

**Pasos:**
1. **Modificar `serve_medgemma.py` para nodo 0:**
   ```python
   # Deployment para nodo 0 con menos memoria
   @serve.deployment(
       name="MedGemma27BNode0",
       num_replicas=1,
       ray_actor_options={
           "num_gpus": 4,
           "resources": {"node_type": "dedicated_27b"}
       }
   )
   class VLLMDeployment27B:
       def __init__(self):
           engine_args = AsyncEngineArgs(
               model="google/medgemma-27b-it",
               tensor_parallel_size=4,
               gpu_memory_utilization=0.85,  # Reducir a 85%
               # ... resto de configuraci√≥n ...
           )
           self.engine = AsyncLLMEngine.from_engine_args(engine_args)
   ```

2. **Instalar y correr Ollama en nodo 0:**
   ```bash
   # Instalar Ollama
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Correr medgemma-4b
   ollama run amsaravi/medgemma-4b-it:q8
   ```

3. **Configurar API para enrutar peticiones:**
   - medgemma-27b ‚Üí vLLM deployment
   - medgemma-4b ‚Üí Ollama API

**Ventajas:**
- ‚úÖ medgemma-27b mantiene 4 GPUs (mejor rendimiento)
- ‚úÖ Ollama es m√°s ligero para modelos peque√±os
- ‚úÖ Separaci√≥n de sistemas

**Desventajas:**
- ‚ö†Ô∏è Requiere Ollama corriendo en paralelo
- ‚ö†Ô∏è Dos sistemas diferentes (vLLM + Ollama)

---

## ‚ö†Ô∏è Consideraciones Importantes

1. **Identificaci√≥n de Nodo 0:**
   - Ray necesita identificar qu√© nodo es el "nodo 0"
   - Usar hostname, IP, o labels de Ray cluster

2. **Routing de Peticiones:**
   - El frontend/API necesita saber a qu√© deployment enviar cada petici√≥n
   - Considerar usar diferentes endpoints o headers

3. **Monitoreo:**
   - Monitorear VRAM en tiempo real despu√©s de implementar
   - Ajustar `gpu_memory_utilization` si es necesario

4. **Pruebas:**
   - Probar con carga real antes de producci√≥n
   - Verificar que no hay OOM errors

---

## üìù Pr√≥ximos Pasos (Sin Implementar A√∫n)

1. ‚úÖ Analizar estrategias (COMPLETADO)
2. ‚è≥ Decidir qu√© estrategia implementar
3. ‚è≥ Modificar `serve_medgemma.py` seg√∫n estrategia elegida
4. ‚è≥ Configurar Ray cluster con labels/placement groups
5. ‚è≥ Modificar routing en frontend/API
6. ‚è≥ Probar y monitorear

