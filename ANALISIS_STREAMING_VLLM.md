# An√°lisis Detallado del Flujo de Streaming vLLM ‚Üí Backend ‚Üí Frontend

## üìä Resumen Ejecutivo

El problema reportado ("hola ¬°ola¬øu√© do rte¬°HHHola") indica que **se est√° concatenando texto acumulado en lugar de deltas**. Este an√°lisis detalla c√≥mo funciona el flujo completo y d√≥nde est√° el problema.

---

## üîÑ Flujo Completo de Streaming

### 1. **vLLM (Servidor de Inferencia)**

**Formato de respuesta:**
```json
data: {"id": "...", "object": "chat.completion.chunk", "choices": [{"index": 0, "delta": {"content": "It"}, "finish_reason": null}]}
data: {"id": "...", "choices": [{"index": 0, "delta": {"content": "Thursday"}, "finish_reason": null}]}
```

**‚úÖ vLLM devuelve DELTAS (solo nuevos tokens)**, no texto acumulado.

**Ubicaci√≥n:** `http://localhost:8000/v1/chat/completions` con `"stream": true`

---

### 2. **LangChain ChatOpenAI (langchain_system.py)**

**C√≥digo relevante:**
```python:49:96:chatbot/langchain_system.py
async def stream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
    """Streaming desde vLLM con Ray Serve - Agrega espacios autom√°ticamente entre tokens"""
    try:
        last_chunk = ""  # √öltimo chunk para detectar si necesita espacio
        accumulated = ""  # Texto acumulado para mejor detecci√≥n
        
        async for chunk in self.ollama_llm.astream(messages, **kwargs):
            chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
            
            if chunk_content:
                # ... l√≥gica de espacios ...
                yield chunk_content
                accumulated += chunk_content
                last_chunk = chunk_content
```

**‚ö†Ô∏è PROBLEMA POTENCIAL:** 
- LangChain's `ChatOpenAI.astream()` **deber√≠a** devolver deltas (solo nuevos tokens)
- Sin embargo, dependiendo de la configuraci√≥n de vLLM y c√≥mo LangChain procesa la respuesta, podr√≠a estar devolviendo texto acumulado
- El c√≥digo actual **asume que son deltas** y los concatena directamente

**Verificaci√≥n necesaria:** Comprobar si `chunk.content` contiene solo el nuevo token o el texto completo acumulado.

---

### 3. **Backend FastAPI (main.py)**

**C√≥digo relevante:**
```python:448:484:chatbot/main.py
async def process_text_stream(message: str, session_id: str):
    """Procesar texto con streaming"""
    try:
        full_response = ""
        start_ts = int(time.time() * 1000)
        async for chunk in medical_chain.stream_chat(message, session_id):
            full_response += chunk
            yield f"data: {json.dumps({'content': chunk, 'done': False})}\n\n"
```

**‚úÖ El backend env√≠a cada chunk tal cual lo recibe de LangChain**

**Formato SSE enviado:**
```
data: {"content": "It", "done": false}

data: {"content": "Thursday", "done": false}
```

---

### 4. **Frontend React (page.tsx)**

**C√≥digo relevante:**
```typescript:266:327:UI_IMSS/app/chat/page.tsx
// Leer stream - usando el mismo patr√≥n que funciona en Quetzalia
const reader = response.body?.getReader()
const decoder = new TextDecoder()
let assistantMessage = ""
let buffer = ""

// Agregar mensaje vac√≠o del asistente
setMessages((prev) => [...prev, { role: "assistant", text: "" }])

if (reader) {
  while (true) {
    const { done, value } = await reader.read()
    
    if (done) break

    buffer += decoder.decode(value, { stream: true })
    const lines = buffer.split("\n")
    buffer = lines.pop() || ""

    for (const line of lines) {
      if (line.startsWith("data: ")) {
        const dataStr = line.slice(6)
        
        if (dataStr.trim() === "[DONE]") {
          continue
        }

        try {
          const data = JSON.parse(dataStr)
          
          if (data.done) {
            break
          }

          if (data.content) {
            assistantMessage += data.content  // ‚ö†Ô∏è AQU√ç EST√Å EL PROBLEMA
            setMessages((prev) => {
              const newMessages = [...prev]
              newMessages[newMessages.length - 1] = {
                role: "assistant",
                text: assistantMessage,
              }
              return newMessages
            })
          }
        } catch (e) {
          console.warn("Error parsing SSE data:", e)
        }
      }
    }
  }
}
```

**‚úÖ El frontend concatena correctamente si recibe deltas**

**‚ùå PROBLEMA:** Si el backend env√≠a texto acumulado, el frontend lo concatena y obtiene duplicaci√≥n:
- Chunk 1: "Hola" ‚Üí `assistantMessage = "Hola"` ‚úÖ
- Chunk 2: "Hola qu√©" ‚Üí `assistantMessage = "Hola" + "Hola qu√©" = "HolaHola qu√©"` ‚ùå
- Chunk 3: "Hola qu√© duele" ‚Üí `assistantMessage = "HolaHola qu√©" + "Hola qu√© duele" = "HolaHola qu√©Hola qu√© duele"` ‚ùå

---

## üîç Diagn√≥stico del Problema

### Hip√≥tesis 1: LangChain est√° devolviendo texto acumulado

**Evidencia:**
- El problema reportado muestra duplicaci√≥n de texto
- El c√≥digo de `FallbackLLM.stream()` asume que son deltas pero no lo verifica

**Soluci√≥n:** Calcular deltas en `FallbackLLM.stream()` antes de enviar al backend.

### Hip√≥tesis 2: vLLM est√° devolviendo texto acumulado (poco probable)

**Evidencia:**
- El curl muestra que vLLM devuelve deltas correctamente
- Pero podr√≠a haber una configuraci√≥n incorrecta

**Soluci√≥n:** Verificar configuraci√≥n de vLLM.

### Hip√≥tesis 3: Problema en el procesamiento de chunks

**Evidencia:**
- El c√≥digo de `stream_chat()` en `langchain_system.py` acumula chunks y luego los env√≠a
- Hay l√≥gica de correcci√≥n de palabras fragmentadas que podr√≠a estar causando problemas

**Soluci√≥n:** Asegurar que solo se env√≠en deltas, no texto acumulado.

---

## ‚úÖ Soluci√≥n Propuesta

### Opci√≥n 1: Calcular Deltas en el Backend (Recomendada)

Modificar `FallbackLLM.stream()` para calcular deltas expl√≠citamente:

```python
async def stream(self, messages: List[BaseMessage], **kwargs) -> AsyncGenerator[str, None]:
    """Streaming desde vLLM - Calcula deltas expl√≠citamente"""
    try:
        previous_text = ""  # Texto acumulado anterior
        
        async for chunk in self.ollama_llm.astream(messages, **kwargs):
            chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
            
            if chunk_content:
                # Calcular delta: solo los nuevos caracteres
                current_text = chunk_content
                
                # Si el chunk es m√°s largo que el anterior, es texto acumulado
                # Si es m√°s corto o igual, es un delta
                if len(current_text) > len(previous_text) and current_text.startswith(previous_text):
                    # Es texto acumulado, calcular delta
                    delta = current_text[len(previous_text):]
                    previous_text = current_text
                else:
                    # Es un delta directo
                    delta = current_text
                    previous_text += delta
                
                if delta:
                    yield delta
```

### Opci√≥n 2: Verificar en LangChain si son deltas o acumulados

Modificar `stream_chat()` para verificar y calcular deltas:

```python
async def stream_chat(self, user_message: str, session_id: str = "") -> AsyncGenerator[str, None]:
    # ... c√≥digo existente ...
    
    previous_text = ""
    async for chunk in self.llm.stream(messages_list):
        if chunk:
            current_text = chunk
            
            # Verificar si es acumulado o delta
            if len(current_text) > len(previous_text) and current_text.startswith(previous_text):
                # Es acumulado, calcular delta
                delta = current_text[len(previous_text):]
                previous_text = current_text
            else:
                # Es delta directo
                delta = current_text
                previous_text += current_text
            
            if delta:
                yield delta
```

### Opci√≥n 3: Usar directamente la API de vLLM (M√°s simple)

En lugar de usar LangChain, llamar directamente a vLLM y procesar los deltas:

```python
async def stream_chat_direct(self, user_message: str, session_id: str = "") -> AsyncGenerator[str, None]:
    """Streaming directo desde vLLM sin LangChain"""
    messages_data = [
        {"role": "system", "content": self.system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    async with httpx.AsyncClient(timeout=120.0) as client:
        async with client.stream(
            "POST",
            f"{self.llm.vllm_endpoint}chat/completions",
            json={
                "model": "google/medgemma-27b-it",
                "messages": messages_data,
                "temperature": 0.7,
                "max_tokens": 100,
                "stream": True
            }
        ) as response:
            if response.status_code == 200:
                async for line in response.aiter_lines():
                    if line.startswith("data: ") and line.strip() != "data: [DONE]":
                        try:
                            data = json.loads(line[6:])
                            if "choices" in data and len(data["choices"]) > 0:
                                delta_content = data["choices"][0].get("delta", {}).get("content", "")
                                if delta_content:
                                    yield delta_content
                        except:
                            pass
```

---

## üß™ C√≥mo Verificar el Problema

1. **Agregar logs en `FallbackLLM.stream()`:**
```python
logger.info(f"üì¶ Chunk recibido: '{chunk_content}' (longitud: {len(chunk_content)})")
logger.info(f"üì¶ Texto acumulado hasta ahora: '{accumulated}'")
```

2. **Verificar si los chunks son deltas o acumulados:**
   - Si cada chunk es m√°s largo que el anterior y contiene el anterior, son acumulados
   - Si cada chunk es independiente y corto, son deltas

3. **Probar directamente con vLLM:**
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "google/medgemma-27b",
    "messages": [{"role": "user", "content": "Hola"}],
    "stream": true
}'
```

---

## üìù Recomendaci√≥n Final

**Implementar Opci√≥n 1** (calcular deltas en `FallbackLLM.stream()`) porque:
1. Es la soluci√≥n m√°s robusta
2. Funciona independientemente de c√≥mo LangChain procese los chunks
3. Garantiza que siempre se env√≠en deltas al frontend
4. No requiere cambios en el frontend

**Implementar Opci√≥n 3** (llamada directa a vLLM) como alternativa si LangChain sigue causando problemas, ya que:
1. Elimina la dependencia de LangChain para streaming
2. Control total sobre el procesamiento de deltas
3. M√°s eficiente (menos capas de abstracci√≥n)





