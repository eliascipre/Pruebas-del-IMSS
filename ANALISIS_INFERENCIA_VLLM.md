# ğŸ“Š AnÃ¡lisis de Arquitectura de Inferencia vLLM con Ray Serve

## ğŸ—ï¸ Arquitectura Actual

### ConfiguraciÃ³n del Servicio Principal

**UbicaciÃ³n**: `/home/administrador/vllm/serve_medgemma.py`

**Modelo**: `google/medgemma-27b-it`

**ConfiguraciÃ³n del Motor**:
```python
engine_args = AsyncEngineArgs(
    model="google/medgemma-27b-it",
    tensor_parallel_size=4,        # Usa 4 GPUs por rÃ©plica
    dtype="bfloat16",
    max_model_len=8192,
    gpu_memory_utilization=0.95,
    enable_lora=False,
    enforce_eager=True,
    trust_remote_code=True,
)
```

**ConfiguraciÃ³n de Deployment**:
- **Autoscaling**: min 2 rÃ©plicas, max 16 rÃ©plicas
- **Target Ongoing Requests**: 5 (escala cuando hay 5+ peticiones en cola)
- **GPUs por RÃ©plica**: 4 GPUs
- **Puerto**: 8000
- **Endpoint**: `http://localhost:8000/v1/chat/completions`

### TopologÃ­a de Red

**Nodo Maestro**: `10.105.20.1`
- Ejecuta Ray Serve y coordina todos los workers
- Puerto 8000 expone la API OpenAI-compatible

**Nodos Workers** (16 nodos):
- `10.105.20.2` hasta `10.105.20.20`
- Cada nodo tiene 4 GPUs NVIDIA A10
- Cada rÃ©plica de vLLM usa las 4 GPUs de un nodo completo

### Estado Actual del Nodo Maestro

SegÃºn `nvidia-smi` mostrado:
```
GPU 0: VLLM::Worker_TP0 - 20730MiB / 23028MiB
GPU 1: VLLM::Worker_TP1 - 20730MiB / 23028MiB
GPU 2: VLLM::Worker_TP2 - 20730MiB / 23028MiB
GPU 3: VLLM::Worker_TP3 - 20730MiB / 23028MiB
```

**AnÃ¡lisis**:
- Cada GPU estÃ¡ usando ~20.7GB de memoria (90% de capacidad)
- El modelo `medgemma-27b-it` estÃ¡ distribuido en 4 GPUs usando Tensor Parallelism
- Las 4 GPUs del nodo maestro estÃ¡n completamente ocupadas por una rÃ©plica

---

## ğŸ” AnÃ¡lisis de Capacidad

### Recursos Disponibles

**Nodo Maestro (10.105.20.1)**:
- 4 GPUs NVIDIA A10
- Cada GPU: 23028MiB memoria total
- Estado: 4 GPUs ocupadas por medgemma-27b-it

**Nodos Workers (10.105.20.2-20)**:
- 16 nodos disponibles
- Cada nodo: 4 GPUs NVIDIA A10
- Capacidad total: 64 GPUs (16 nodos Ã— 4 GPUs)

### Opciones para Cargar un Segundo Modelo

#### OpciÃ³n 1: Usar un Nodo Worker Dedicado (RECOMENDADO)

**Ventajas**:
- âœ… No afecta el rendimiento del modelo principal
- âœ… Aislamiento completo de recursos
- âœ… Escalado independiente
- âœ… ConfiguraciÃ³n mÃ¡s simple

**ConfiguraciÃ³n**:
- Asignar un nodo worker especÃ­fico (ej: `10.105.20.2`) para el segundo modelo
- Crear un segundo deployment de Ray Serve en el mismo cluster
- Usar un puerto diferente (ej: 8001)

#### OpciÃ³n 2: Compartir GPUs en el Nodo Maestro (NO RECOMENDADO)

**Desventajas**:
- âŒ Requiere reducir `tensor_parallel_size` a 2 para cada modelo
- âŒ Menor rendimiento por modelo
- âŒ Posible fragmentaciÃ³n de memoria
- âŒ Mayor latencia

**ConfiguraciÃ³n**:
- `tensor_parallel_size=2` para medgemma-27b-it (2 GPUs)
- `tensor_parallel_size=2` para segundo modelo (2 GPUs restantes)

#### OpciÃ³n 3: Usar un Nodo Worker con GPU Selection (INTERMEDIO)

**Ventajas**:
- âœ… Mantiene el rendimiento del modelo principal
- âœ… Flexibilidad para escalar

**ConfiguraciÃ³n**:
- Usar Ray con selecciÃ³n especÃ­fica de GPUs
- Asignar GPUs especÃ­ficas a cada deployment

---

## ğŸ¯ RecomendaciÃ³n: Cargar NV-Reason-CXR en un Nodo Worker

### Modelo Objetivo: NV-Reason-CXR-3B

**CaracterÃ­sticas**:
- Modelo mÃ¡s pequeÃ±o: 3B parÃ¡metros vs 27B de MedGemma
- Puede funcionar con 1-2 GPUs (no requiere 4)
- Especializado en anÃ¡lisis de radiografÃ­as de tÃ³rax

### ConfiguraciÃ³n Propuesta

**OpciÃ³n A: Deployment Independiente en Ray Serve** (Mejor para integraciÃ³n)

Crear un segundo deployment en el mismo cluster Ray:
- **Puerto**: 8001
- **Nodo asignado**: `10.105.20.2` (o cualquier worker disponible)
- **Tensor Parallelism**: 2 GPUs (suficiente para 3B)
- **Endpoint**: `http://10.105.20.1:8001/v1/chat/completions`

**OpciÃ³n B: Servicio Separado con Transformers** (Actual)

Mantener el servicio actual en `nv-reason-cxr` que usa transformers directamente:
- **Puerto**: 5005
- **Ventaja**: No requiere Ray Serve, mÃ¡s simple
- **Desventaja**: No aprovecha la optimizaciÃ³n de vLLM

---

## ğŸ“ Plan de ImplementaciÃ³n

### Paso 1: Crear Deployment para NV-Reason-CXR con vLLM

1. Crear archivo `serve_nv_reason.py` similar a `serve_medgemma.py`
2. Configurar con `tensor_parallel_size=2` (2 GPUs son suficientes)
3. Asignar a un nodo worker especÃ­fico
4. Exponer en puerto 8001

### Paso 2: Integrar con el Chatbot

1. Modificar `chatbot/langchain_system.py` para usar el nuevo endpoint
2. Agregar fallback automÃ¡tico entre modelos
3. Actualizar `medical_analysis.py` para routing inteligente

### Paso 3: Configurar Load Balancing

1. Usar Ray Serve para distribuir carga entre modelos
2. Implementar routing basado en tipo de consulta:
   - Texto general â†’ MedGemma 27B (puerto 8000)
   - RadiografÃ­as â†’ NV-Reason-CXR (puerto 8001)

---

## ğŸ”§ ConfiguraciÃ³n TÃ©cnica Detallada

### Arquitectura de Red

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nodo Maestro (10.105.20.1)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ray Serve Controller                             â”‚  â”‚
â”‚  â”‚  - MedGemma Deployment (puerto 8000)              â”‚  â”‚
â”‚  â”‚  - NV-Reason Deployment (puerto 8001)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  GPUs 0-3: MedGemma 27B (TP=4)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â†’ Worker 1 (10.105.20.2)
         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   â”‚  GPUs 0-1: NV-Reason-CXR (TP=2)        â”‚
         â”‚   â”‚  GPUs 2-3: Disponibles                â”‚
         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â†’ Worker 2-16 (10.105.20.3-20)
         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   â”‚  GPUs 0-3: MedGemma Workers (TP=4)    â”‚
         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memoria Estimada

**MedGemma 27B**:
- 4 GPUs Ã— 20.7GB = ~83GB total
- bfloat16: ~54GB modelo + ~29GB activaciones/KV cache

**NV-Reason-CXR 3B**:
- 2 GPUs Ã— ~10GB = ~20GB total
- bfloat16: ~6GB modelo + ~14GB activaciones/KV cache

**Capacidad disponible**:
- Nodo Worker tÃ­pico: 4 GPUs Ã— 23GB = 92GB
- NV-Reason usarÃ­a: ~20GB (22% de capacidad)
- Espacio restante: ~72GB (disponible para otro modelo pequeÃ±o)

---

## âœ… ConclusiÃ³n

**Respuesta a tu pregunta**: SÃ­, es posible cargar otro modelo en el nodo 1, pero **NO es recomendable** porque:

1. El nodo maestro ya estÃ¡ usando las 4 GPUs para MedGemma 27B
2. Compartir GPUs reducirÃ­a el rendimiento de ambos modelos
3. Mejor usar un nodo worker dedicado (ej: `10.105.20.2`)

**RecomendaciÃ³n Final**:
- **Mantener MedGemma 27B** en el nodo maestro (4 GPUs)
- **Cargar NV-Reason-CXR** en un nodo worker (2 GPUs suficientes)
- **Crear segundo deployment** en Ray Serve para mejor integraciÃ³n
- **Mantener servicio actual** en puerto 5005 como fallback

