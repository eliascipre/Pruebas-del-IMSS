# An√°lisis de VRAM y Viabilidad de Modelos

**Fecha:** 8 de Noviembre 2025, 20:31  
**Estado del Sistema:** Modelo vLLM activo con tensor_parallel_size=4

## üìä Estado Actual de VRAM

### Consumo de VRAM por Nodo

**Tu nodo tiene 4 GPUs (NVIDIA A10 con 23 GB cada una)**

| Componente | Consumo por GPU | Consumo Total (4 GPUs) |
|------------|-----------------|------------------------|
| **Workers vLLM** | ~20,730 MiB (~20.2 GB) | **82,920 MiB (~80.98 GB)** |
| **Proceso Python** | ~500 MiB | **~2,000 MiB (~1.95 GB)** |
| **Total por Nodo** | ~21,200 MiB (~20.7 GB) | **~84,920 MiB (~82.9 GB)** |

**Resumen:**
- **Consumo por nodo (4 GPUs):** ~82.9 GB
- **Consumo por r√©plica:** ~81 GB (1 r√©plica usa 4 GPUs con `tensor_parallel_size=4`)
- **Consumo por GPU individual:** ~20.7 GB

### Distribuci√≥n de Memoria por GPU

| GPU | Memoria Usada | Memoria Total | Memoria Libre | Estado |
|-----|---------------|---------------|---------------|--------|
| 0   | 22,483 MiB    | 23,028 MiB    | **32 MiB**    | üî¥ Cr√≠tico |
| 1   | 21,259 MiB    | 23,028 MiB    | **1,256 MiB** | üü° Bajo |
| 2   | 21,261 MiB    | 23,028 MiB    | **1,254 MiB** | üü° Bajo |
| 3   | 21,151 MiB    | 23,028 MiB    | **1,364 MiB** | üü° Bajo |
| **Total** | **86,154 MiB** | **92,112 MiB** | **~3,906 MiB (~3.8 GB)** | ‚ö†Ô∏è |

### Procesos Activos

- **Modelo vLLM corriendo:** `tensor_parallel_size=4`
  - Worker_TP0: 20,730 MiB (GPU 0)
  - Worker_TP1: 20,730 MiB (GPU 1)
  - Worker_TP2: 20,730 MiB (GPU 2)
  - Worker_TP3: 20,730 MiB (GPU 3)
  - **Total workers vLLM:** 82,920 MiB (~80.98 GB)
- **Proceso Python adicional:** ~2,000 MiB distribuido

## ü§î ¬øEs Posible Correr los Modelos de Ollama?

### 1. `medgemma-27b-it:q8` (29 GB)

**‚ùå NO ES POSIBLE** con la configuraci√≥n actual

**Razones:**
- Requiere ~29 GB de VRAM libre
- Solo hay ~3.9 GB disponibles
- Necesitar√≠as liberar ~25 GB adicionales
- Con `tensor_parallel_size=4`, se distribuir√≠a en 4 GPUs (~7.25 GB por GPU)
- Cada GPU solo tiene ~1.2 GB libre

**Soluci√≥n:**
- Detener completamente el modelo actual
- Liberar toda la VRAM
- Luego cargar el modelo de 27B

### 2. `medgemma-4b-it:q8` (5.0 GB)

**‚ö†Ô∏è T√âCNICAMENTE POSIBLE, pero requiere cambios**

**Requisitos:**
- Necesita ~5.0 GB de VRAM libre
- Con `tensor_parallel_size=4`: ~1.25 GB por GPU
- Con `gpu_memory_utilization=0.95`: ~5.2 GB totales necesarios

**Problemas actuales:**
- Solo hay ~3.9 GB libres (insuficiente)
- GPU 0 tiene solo 32 MiB libres (cr√≠tico)

**Soluciones:**

#### Opci√≥n A: Detener modelo actual y cargar el 4B
```bash
# Detener el servicio actual
# Luego cargar medgemma-4b-it:q8
```

#### Opci√≥n B: Ajustar configuraci√≥n para modelo m√°s peque√±o
- Reducir `gpu_memory_utilization` a 0.85-0.90
- Considerar `tensor_parallel_size=2` (2 GPUs en lugar de 4)
- Esto liberar√≠a 2 GPUs para otros usos

## üìà Comportamiento de VRAM con M√°s Peticiones

### Respuesta Corta: **NO aumenta el consumo de VRAM por petici√≥n**

### Explicaci√≥n Detallada:

El consumo de VRAM en vLLM es **fijo por r√©plica**, no por petici√≥n:

1. **Carga del Modelo (una vez por r√©plica):**
   - Cada r√©plica carga el modelo completo en memoria
   - Esto consume ~20.7 GB √ó 4 GPUs = ~82.8 GB por r√©plica
   - **Este consumo NO cambia** con m√°s peticiones

2. **Procesamiento de Peticiones:**
   - Las peticiones se procesan en **colas** dentro de la misma r√©plica
   - vLLM usa **PagedAttention** para manejar m√∫ltiples peticiones eficientemente
   - El consumo adicional es **m√≠nimo** (solo tokens activos)

3. **Autoscaling:**
   - Con m√°s peticiones, Ray Serve crea **nuevas r√©plicas**
   - Cada r√©plica nueva consume ~82.8 GB adicionales
   - Pero esto es **escalado horizontal**, no aumento de VRAM por petici√≥n

### Configuraci√≥n Actual del Autoscaling:

```python
autoscaling_config={
    "min_replicas": 2,     # 2 r√©plicas siempre activas = ~165.6 GB
    "max_replicas": 16,    # Hasta 16 r√©plicas = ~1,324.8 GB
    "target_ongoing_requests": 5  # Nueva r√©plica si hay 5+ peticiones en cola
}
```

**Implicaciones:**
- **Con 1 r√©plica:** ~82.8 GB (4 GPUs)
- **Con 2 r√©plicas (m√≠nimo):** ~165.6 GB (8 GPUs)
- **Con 16 r√©plicas (m√°ximo):** ~1,324.8 GB (64 GPUs)

**‚ö†Ô∏è IMPORTANTE:** Con tu configuraci√≥n actual, el autoscaling **NO puede funcionar** porque:
- Solo tienes 4 GPUs disponibles
- Cada r√©plica necesita 4 GPUs (`num_gpus=4`)
- Solo puedes tener **1 r√©plica activa** con 4 GPUs

## üîß Recomendaciones

### Para Correr `medgemma-4b-it:q8`:

1. **Detener el modelo actual:**
   ```bash
   # Detener el servicio vLLM actual
   ```

2. **Ajustar configuraci√≥n para modelo m√°s peque√±o:**
   ```python
   engine_args = AsyncEngineArgs(
       model="amsaravi/medgemma-4b-it:q8",  # Modelo m√°s peque√±o
       tensor_parallel_size=2,  # Reducir a 2 GPUs
       dtype="bfloat16",
       max_model_len=8192,
       gpu_memory_utilization=0.85,  # Reducir a 85% para margen
       # ... resto de configuraci√≥n
   )
   ```

3. **Ajustar autoscaling:**
   ```python
   autoscaling_config={
       "min_replicas": 1,     # 1 r√©plica m√≠nima
       "max_replicas": 2,     # M√°ximo 2 r√©plicas (4 GPUs / 2 GPUs por r√©plica)
       "target_ongoing_requests": 3
   },
   ray_actor_options={
       "num_gpus": 2,  # Cambiar a 2 GPUs por r√©plica
   }
   ```

### Para Optimizar el Modelo Actual (27B):

1. **Reducir `gpu_memory_utilization`:**
   ```python
   gpu_memory_utilization=0.90,  # De 0.95 a 0.90
   ```
   Esto liberar√≠a ~1 GB por GPU (~4 GB total)

2. **Ajustar autoscaling:**
   ```python
   autoscaling_config={
       "min_replicas": 1,     # Solo 1 r√©plica (no puedes tener m√°s con 4 GPUs)
       "max_replicas": 1,     # M√°ximo 1 r√©plica
       "target_ongoing_requests": 10  # Aumentar umbral
   }
   ```

3. **Considerar `tensor_parallel_size=2`:**
   - Esto permitir√≠a 2 r√©plicas simult√°neas
   - Pero cada r√©plica ser√≠a m√°s lenta
   - √ötil si tienes muchas peticiones concurrentes

## üìù Resumen

| Modelo | Tama√±o | VRAM Necesaria | VRAM Disponible | ¬øPosible? |
|--------|--------|----------------|-----------------|-----------|
| medgemma-27b-it:q8 | 29 GB | ~29 GB | ~3.9 GB | ‚ùå No |
| medgemma-4b-it:q8 | 5.0 GB | ~5.2 GB | ~3.9 GB | ‚ö†Ô∏è Con cambios |

**Consumo de VRAM con m√°s peticiones:**
- ‚úÖ **NO aumenta** por petici√≥n
- ‚úÖ **Fijo por r√©plica** (~82.8 GB por r√©plica)
- ‚ö†Ô∏è **Aumenta con nuevas r√©plicas** (autoscaling)

**Recomendaci√≥n final:**
- Para correr `medgemma-4b-it:q8`: Detener modelo actual y ajustar configuraci√≥n
- Para optimizar modelo actual: Reducir `gpu_memory_utilization` y ajustar autoscaling

