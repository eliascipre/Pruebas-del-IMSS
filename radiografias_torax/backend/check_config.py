#!/usr/bin/env python3
"""
Script r√°pido para verificar la configuraci√≥n de LM Studio
"""

import requests
import json

def check_lm_studio():
    """Verifica que LM Studio est√© corriendo y configurado correctamente"""
    print("üîç Verificando configuraci√≥n de LM Studio...")
    
    try:
        # Verificar que el servidor est√© corriendo
        response = requests.get("http://localhost:1234/v1/models", timeout=5)
        
        if response.status_code == 200:
            print("‚úÖ LM Studio est√° corriendo en localhost:1234")
            
            # Mostrar modelos disponibles
            data = response.json()
            models = data.get('data', [])
            
            if models:
                print(f"üìã Modelos disponibles ({len(models)}):")
                for model in models:
                    model_id = model.get('id', 'unknown')
                    print(f"   - {model_id}")
            else:
                print("‚ö†Ô∏è No hay modelos cargados")
                
            return True
        else:
            print(f"‚ùå LM Studio respondi√≥ con c√≥digo {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("‚ùå No se puede conectar a LM Studio")
        print("   Aseg√∫rate de que est√© corriendo en localhost:1234")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def check_medgemma():
    """Verifica si MedGemma est√° disponible"""
    print("\nüîç Verificando MedGemma...")
    
    try:
        response = requests.get("http://localhost:1234/v1/models", timeout=5)
        data = response.json()
        models = data.get('data', [])
        
        medgemma_models = [m for m in models if 'medgemma' in m.get('id', '').lower()]
        
        if medgemma_models:
            print("‚úÖ MedGemma est√° disponible:")
            for model in medgemma_models:
                print(f"   - {model.get('id')}")
            return True
        else:
            print("‚ö†Ô∏è MedGemma no est√° cargado")
            print("   Descarga MedGemma-4B desde Hugging Face y c√°rgalo en LM Studio")
            return False
            
    except Exception as e:
        print(f"‚ùå Error verificando MedGemma: {e}")
        return False

if __name__ == "__main__":
    print("üöÄ Verificador de configuraci√≥n LM Studio\n")
    
    lm_ok = check_lm_studio()
    medgemma_ok = check_medgemma()
    
    print("\nüìä Resumen:")
    print(f"   LM Studio: {'‚úÖ OK' if lm_ok else '‚ùå FALLO'}")
    print(f"   MedGemma: {'‚úÖ OK' if medgemma_ok else '‚ùå FALLO'}")
    
    if lm_ok and medgemma_ok:
        print("\nüéâ ¬°Configuraci√≥n correcta! Puedes ejecutar el backend.")
    else:
        print("\n‚ö†Ô∏è Revisa la configuraci√≥n antes de continuar.")
