"""
Sistema de an√°lisis m√©dico para im√°genes - Ollama para im√°genes, vLLM para texto
Usa Ollama (medgemma-4b) para an√°lisis de im√°genes manteniendo toda la arquitectura Langchain
"""

import base64
import logging
import os
import httpx
import asyncio
from typing import Dict, Any, Optional
import json
from PIL import Image
import io

logger = logging.getLogger(__name__)

# Configuraci√≥n para Ollama (im√°genes)
OLLAMA_ENDPOINT = os.getenv("OLLAMA_ENDPOINT", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_IMAGE_MODEL", "amsaravi/medgemma-4b-it:q8")

# Configuraci√≥n para vLLM (texto) - mantener para compatibilidad
VLLM_ENDPOINT = os.getenv("VLLM_ENDPOINT", os.getenv("LM_STUDIO_ENDPOINT", "http://localhost:8000/v1/"))
if not VLLM_ENDPOINT.endswith("/v1/"):
    if VLLM_ENDPOINT.endswith("/"):
        VLLM_ENDPOINT = VLLM_ENDPOINT + "v1/"
    else:
        VLLM_ENDPOINT = VLLM_ENDPOINT + "/v1/"
MODEL_NAME = "google/medgemma-27b-it"

# L√≠mites para im√°genes
MAX_IMAGE_SIZE_MB = 2  # 2MB m√°ximo en bytes originales
MAX_IMAGE_DIMENSION = 512  # M√°ximo 512px en cualquier dimensi√≥n
MAX_IMAGE_QUALITY = 85  # Calidad JPEG (0-100)
MAX_IMAGE_TOKENS = 15000  # Tokens m√°ximo para imagen en base64 (aumentado para permitir im√°genes m√©dicas m√°s grandes)


def compress_image(image_data: str, max_dimension: int = MAX_IMAGE_DIMENSION, quality: int = MAX_IMAGE_QUALITY) -> str:
    """Comprimir imagen a tama√±o m√°ximo y calidad para reducir tokens"""
    try:
        # Decodificar base64
        image_bytes = base64.b64decode(image_data)
        original_size = len(image_bytes)
        
        # Abrir imagen
        image = Image.open(io.BytesIO(image_bytes))
        original_mode = image.mode
        
        # Convertir a RGB si es necesario (JPEG solo soporta RGB)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Redimensionar si es muy grande
        if max(image.size) > max_dimension:
            image.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)
            logger.info(f"üìê Imagen redimensionada a {image.size}")
        
        # Comprimir a JPEG
        output = io.BytesIO()
        image.save(output, format='JPEG', quality=quality, optimize=True)
        output.seek(0)
        
        # Codificar a base64
        compressed_base64 = base64.b64encode(output.read()).decode('utf-8')
        compressed_size = len(compressed_base64)
        
        # Calcular reducci√≥n
        reduction = ((original_size - len(base64.b64decode(compressed_base64))) / original_size) * 100 if original_size > 0 else 0
        
        logger.info(f"üì¶ Imagen comprimida: {original_size / 1024:.1f}KB ‚Üí {len(base64.b64decode(compressed_base64)) / 1024:.1f}KB ({reduction:.1f}% reducci√≥n)")
        logger.info(f"üìä Tama√±o base64: {len(image_data)} ‚Üí {compressed_size} caracteres")
        
        return compressed_base64
    except Exception as e:
        logger.error(f"‚ùå Error comprimiendo imagen: {e}")
        # Si falla la compresi√≥n, devolver la imagen original
        return image_data


def validate_image_size(image_data: str) -> tuple[bool, Optional[str]]:
    """Validar que la imagen no exceda el l√≠mite de tokens"""
    try:
        # Decodificar para obtener tama√±o en bytes
        image_bytes = base64.b64decode(image_data)
        size_mb = len(image_bytes) / (1024 * 1024)
        
        # Validar tama√±o en MB
        if size_mb > MAX_IMAGE_SIZE_MB:
            return False, f"Imagen muy grande ({size_mb:.2f}MB). M√°ximo permitido: {MAX_IMAGE_SIZE_MB}MB"
        
        # Estimar tokens (base64 es ~33% m√°s grande que bytes originales)
        # Aproximaci√≥n: 4 caracteres base64 = 1 token
        estimated_tokens = len(image_data) // 4
        
        if estimated_tokens > MAX_IMAGE_TOKENS:
            return False, f"Imagen excede l√≠mite de tokens estimados ({estimated_tokens}). M√°ximo: {MAX_IMAGE_TOKENS} tokens"
        
        logger.info(f"‚úÖ Imagen validada: {size_mb:.2f}MB, ~{estimated_tokens} tokens estimados")
        return True, None
    except Exception as e:
        logger.error(f"‚ùå Error validando imagen: {e}")
        return False, f"Error validando imagen: {str(e)}"


class MedicalImageAnalysis:
    """Sistema de an√°lisis de im√°genes m√©dicas con Ollama (medgemma-4b) manteniendo arquitectura Langchain"""
    
    def __init__(self, system_prompt: Optional[str] = None):
        self.system_prompt = system_prompt
        logger.info(f"‚úÖ Configurado para usar Ollama en: {OLLAMA_ENDPOINT}")
        logger.info(f"‚úÖ Modelo de im√°genes: {OLLAMA_MODEL}")
    
    def _load_medical_prompt(self) -> str:
        """Cargar prompt m√©dico desde archivo (igual que langchain_system.py)"""
        try:
            prompt_path = os.path.join(os.path.dirname(__file__), 'prompts', 'medico.md')
            with open(prompt_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # Extraer el system prompt principal
                if '## System Prompt Principal' in content:
                    start = content.find('## System Prompt Principal') + len('## System Prompt Principal')
                    end = content.find('##', start)
                    if end > start:
                        return content[start:end].strip()
                return content
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error cargando prompt m√©dico: {e}")
        
        # Fallback
        return """Eres un asistente m√©dico especializado del IMSS que proporciona informaci√≥n m√©dica general, 
interpretaci√≥n de s√≠ntomas y gu√≠as de salud preventiva. 

IMPORTANTE: Siempre recomiendas consultar con profesionales de la salud del IMSS para diagn√≥sticos espec√≠ficos 
y tratamientos m√©dicos. Responde en espa√±ol."""
    
    async def analyze_with_ollama(self, image_data: str, prompt: str, session_id: Optional[str] = None, 
                                  conversation_history: Optional[list] = None, 
                                  entity_context: Optional[str] = None,
                                  abort_controller: Optional[Any] = None) -> Dict[str, Any]:
        """
        Analizar imagen usando Ollama (medgemma-4b) manteniendo toda la arquitectura Langchain
        
        Args:
            image_data: Imagen en base64
            prompt: Prompt del usuario
            session_id: ID de sesi√≥n para contexto
            conversation_history: Historial de conversaci√≥n (opcional)
            entity_context: Contexto de entidades extra√≠das (opcional)
        """
        try:
            logger.info(f"ü§ñ Analizando con Ollama: {OLLAMA_MODEL}")
            
            # Validar tama√±o de imagen
            is_valid, error_msg = validate_image_size(image_data)
            if not is_valid:
                logger.warning(f"‚ö†Ô∏è {error_msg}. Comprimiendo imagen...")
                # Comprimir imagen si es muy grande (primera compresi√≥n con calidad 85)
                image_data = compress_image(image_data, quality=85)
                # Validar nuevamente despu√©s de compresi√≥n
                is_valid, error_msg = validate_image_size(image_data)
                if not is_valid:
                    # Si a√∫n es muy grande, comprimir m√°s agresivamente (calidad 70)
                    logger.warning(f"‚ö†Ô∏è Imagen a√∫n muy grande despu√©s de primera compresi√≥n. Comprimiendo m√°s agresivamente...")
                    image_data = compress_image(image_data, quality=70, max_dimension=512)
                    # Validar nuevamente despu√©s de segunda compresi√≥n
                    is_valid, error_msg = validate_image_size(image_data)
                    if not is_valid:
                        # Si a√∫n es muy grande, comprimir a√∫n m√°s agresivamente (calidad 60, dimensi√≥n 400)
                        logger.warning(f"‚ö†Ô∏è Imagen a√∫n muy grande despu√©s de segunda compresi√≥n. Comprimiendo muy agresivamente...")
                        image_data = compress_image(image_data, quality=60, max_dimension=400)
                        # Validar nuevamente despu√©s de tercera compresi√≥n
                        is_valid, error_msg = validate_image_size(image_data)
                        if not is_valid:
                            return {
                                "success": False,
                                "error": f"Imagen demasiado grande incluso despu√©s de compresi√≥n agresiva: {error_msg}",
                                "provider": "ollama"
                            }
            
            # Decodificar imagen para obtener metadata
            image_bytes = base64.b64decode(image_data)
            image_size = len(image_bytes)
            
            # Cargar system prompt (igual que langchain_system.py)
            system_prompt = self.system_prompt or self._load_medical_prompt()
            
            # Construir prompt completo con contexto de entidades si existe
            full_system_prompt = system_prompt
            if entity_context:
                full_system_prompt = f"{system_prompt}\n\n{entity_context}"
            
            # Agregar contexto de historial si existe
            if conversation_history and len(conversation_history) > 0:
                # Tomar √∫ltimos 3 mensajes del historial
                recent_history = conversation_history[-3:] if len(conversation_history) > 3 else conversation_history
                history_text = "\n\n## Contexto de la conversaci√≥n:\n"
                for msg in recent_history:
                    if hasattr(msg, 'content'):
                        role = "Usuario" if hasattr(msg, '__class__') and 'Human' in str(type(msg)) else "Asistente"
                        history_text += f"{role}: {msg.content}\n"
                full_system_prompt = f"{full_system_prompt}\n{history_text}"
            
            # Construir prompt final para an√°lisis de imagen
            analysis_prompt = f"""{full_system_prompt}

IMPORTANTE: El usuario ha compartido una radiograf√≠a/imagen m√©dica.
Analiza la imagen proporcionada y proporciona:
1. Descripci√≥n de estructuras anat√≥micas visibles
2. Hallazgos normales vs anormales
3. Posibles patolog√≠as o alteraciones
4. Recomendaciones profesionales
5. Siempre remitir a consulta m√©dica del IMSS para confirmaci√≥n

Prompt del usuario: {prompt if prompt else 'Analiza esta radiograf√≠a m√©dica en detalle'}"""
            
            logger.info(f"üìè Enviando imagen a Ollama (tama√±o: {image_size} bytes)")
            logger.info(f"üìù Prompt del usuario: {prompt[:100] if prompt else 'Sin prompt'}...")
            
            # Preparar payload para Ollama (formato del script de ejemplo)
            # Formato: {"model": "...", "prompt": "...", "images": [base64_image], "stream": False}
            payload = {
                "model": OLLAMA_MODEL,
                "prompt": analysis_prompt,
                "images": [image_data],  # Array de strings base64
                "stream": False
            }
            
            # Enviar petici√≥n a Ollama con soporte para cancelaci√≥n
            timeout = httpx.Timeout(600.0, connect=10.0)
            async with httpx.AsyncClient(timeout=timeout) as client:  # 10 minutos de timeout para an√°lisis de im√°genes
                logger.info(f"üöÄ Enviando imagen a {OLLAMA_ENDPOINT}/api/generate")
                
                # Verificar si fue cancelado antes de enviar
                if abort_controller and abort_controller.signal.aborted:
                    logger.info("üõë Request cancelado antes de enviar a Ollama")
                    return {
                        "success": False,
                        "error": "Request cancelado por el usuario",
                        "provider": "ollama",
                        "cancelled": True
                    }
                
                try:
                    response = await client.post(
                        f"{OLLAMA_ENDPOINT}/api/generate",
                        json=payload
                    )
                except asyncio.CancelledError:
                    logger.info("üõë Request cancelado durante env√≠o a Ollama")
                    return {
                        "success": False,
                        "error": "Request cancelado por el usuario",
                        "provider": "ollama",
                        "cancelled": True
                    }
                
                if response.status_code == 200:
                    result = response.json()
                    analysis = result.get('response', '')
                    
                    if not analysis:
                        logger.error(f"‚ùå No se recibi√≥ respuesta del modelo")
                        logger.warning(f"Respuesta completa: {result}")
                        return {
                            "success": False,
                            "error": "No se recibi√≥ respuesta del modelo",
                            "provider": "ollama"
                        }
                    
                    # Logging detallado de la respuesta
                    logger.info(f"‚úÖ Ollama response recibida (an√°lisis de imagen)")
                    logger.info(f"üìù Respuesta del modelo (primeros 200 caracteres): {analysis[:200]}...")
                    logger.info(f"üìä Tama√±o de la respuesta: {len(analysis)} caracteres")
                    
                    # Verificar si la respuesta parece ser un an√°lisis real o solo texto gen√©rico
                    if len(analysis) < 50:
                        logger.warning(f"‚ö†Ô∏è Respuesta muy corta ({len(analysis)} caracteres). El modelo puede no estar procesando la imagen correctamente.")
                    
                    return {
                        "success": True,
                        "analysis": analysis,
                        "model": OLLAMA_MODEL,
                        "provider": "ollama"
                    }
                else:
                    error_text = response.text[:2000]  # Limitar tama√±o del error
                    logger.error(f"‚ùå Error en Ollama: {response.status_code}")
                    logger.error(f"üìã Detalles del error: {error_text}")
                    
                    # Intentar parsear error si es JSON
                    try:
                        error_json = json.loads(error_text)
                        error_detail = error_json.get('error', {}).get('message', error_text) if isinstance(error_json.get('error'), dict) else error_text
                    except:
                        error_detail = error_text
                    
                    return {
                        "success": False,
                        "error": f"Error del servidor ({response.status_code}): {error_detail[:500]}",
                        "provider": "ollama"
                    }
        except httpx.TimeoutException:
            logger.error(f"‚ùå Timeout esperando respuesta de Ollama (m√°s de 10 minutos)")
            return {
                "success": False,
                "error": "Timeout esperando respuesta del servidor. El an√°lisis de im√°genes puede tardar varios minutos.",
                "provider": "ollama"
            }
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis con Ollama: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "provider": "ollama"
            }
    
    async def analyze_with_fallback(self, image_data: str, image_format: str, prompt: str, 
                                     session_id: Optional[str] = None,
                                     conversation_history: Optional[list] = None,
                                     entity_context: Optional[str] = None,
                                     abort_controller: Optional[Any] = None) -> Dict[str, Any]:
        """An√°lisis de imagen con Ollama (medgemma-4b) manteniendo arquitectura Langchain"""
        return await self.analyze_with_ollama(image_data, prompt, session_id, conversation_history, entity_context, abort_controller)


# Instancia global (se inicializar√° con system_prompt cuando se necesite)
_medical_analyzer: Optional[MedicalImageAnalysis] = None


def get_medical_analyzer(system_prompt: Optional[str] = None) -> MedicalImageAnalysis:
    """Obtener instancia del analizador m√©dico (singleton)"""
    global _medical_analyzer
    if _medical_analyzer is None:
        _medical_analyzer = MedicalImageAnalysis(system_prompt=system_prompt)
    elif system_prompt and not _medical_analyzer.system_prompt:
        _medical_analyzer.system_prompt = system_prompt
    return _medical_analyzer


async def analyze_image_with_fallback(image_data: str, image_format: str, prompt: str, 
                                       session_id: Optional[str] = None,
                                       conversation_history: Optional[list] = None,
                                       entity_context: Optional[str] = None,
                                       system_prompt: Optional[str] = None,
                                       abort_controller: Optional[Any] = None) -> Dict[str, Any]:
    """
    Funci√≥n helper para an√°lisis de imagen con Ollama manteniendo arquitectura Langchain
    
    Args:
        image_data: Imagen en base64
        image_format: Formato de la imagen (jpeg, png, etc.)
        prompt: Prompt del usuario
        session_id: ID de sesi√≥n para contexto
        conversation_history: Historial de conversaci√≥n (opcional)
        entity_context: Contexto de entidades extra√≠das (opcional)
        system_prompt: System prompt personalizado (opcional)
    """
    analyzer = get_medical_analyzer(system_prompt=system_prompt)
    return await analyzer.analyze_with_fallback(image_data, image_format, prompt, session_id, conversation_history, entity_context, abort_controller)
