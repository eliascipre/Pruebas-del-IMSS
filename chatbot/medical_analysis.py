"""
Sistema de an√°lisis m√©dico para im√°genes - SOLO LM Studio
"""

import base64
import logging
import os
import httpx
from typing import Dict, Any, Optional
import json

logger = logging.getLogger(__name__)

# Configuraci√≥n
LM_STUDIO_ENDPOINT = os.getenv("LM_STUDIO_ENDPOINT", "http://localhost:1234/v1/")
MODEL_NAME = "medgemma-4b-it-mlx"


class MedicalImageAnalysis:
    """Sistema de an√°lisis de im√°genes m√©dicas con LM Studio"""
    
    def __init__(self):
        logger.info(f"‚úÖ Configurado para usar LM Studio en: {LM_STUDIO_ENDPOINT}")
    
    async def analyze_with_lm_studio(self, image_data: str, prompt: str) -> Dict[str, Any]:
        """Analizar imagen usando LM Studio local con formato multimodal"""
        try:
            logger.info(f"ü§ñ Analizando con LM Studio: {MODEL_NAME}")
            
            # Decodificar imagen para obtener metadata
            image_bytes = base64.b64decode(image_data)
            image_size = len(image_bytes)
            
            # Crear el prompt m√©dico mejorado
            system_prompt = """Eres un radi√≥logo asistente especializado del IMSS.
Analiza la imagen m√©dica proporcionada y proporciona:
1. Descripci√≥n de estructuras anat√≥micas visibles
2. Hallazgos normales y anormales
3. Posibles patolog√≠as o alteraciones
4. Recomendaciones profesionales
5. Siempre remitir a consulta m√©dica para confirmaci√≥n

Responde en espa√±ol de manera detallada y profesional."""
            
            # Preparar mensaje multimodal seg√∫n formato LangChain
            user_prompt_text = prompt if prompt else "Analiza esta radiograf√≠a m√©dica en detalle"
            
            # Formato multimodal: content es un array
            multimodal_content = [
                {
                    "type": "text",
                    "text": user_prompt_text
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_data}"
                    }
                }
            ]
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": multimodal_content}
            ]
            
            logger.info(f"üìè Enviando imagen (tama√±o: {image_size} bytes) con formato multimodal")
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{LM_STUDIO_ENDPOINT}chat/completions",
                    json={
                        "model": MODEL_NAME,
                        "messages": messages,
                        "temperature": 0.7,
                        "max_tokens": -1,
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    analysis = result["choices"][0]["message"]["content"]
                    
                    logger.info(f"‚úÖ LM Studio response recibida (an√°lisis multimodal)")
                    return {
                        "success": True,
                        "analysis": analysis,
                        "model": MODEL_NAME,
                        "provider": "lm_studio"
                    }
                else:
                    error_text = response.text
                    logger.error(f"‚ùå Error en LM Studio: {response.status_code} - {error_text}")
                    return {
                        "success": False,
                        "error": f"HTTP {response.status_code}: {error_text}",
                        "provider": "lm_studio"
                    }
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis con LM Studio: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "provider": "lm_studio"
            }
    
    async def analyze_with_fallback(self, image_data: str, image_format: str, prompt: str) -> Dict[str, Any]:
        """An√°lisis de imagen con LM Studio"""
        return await self.analyze_with_lm_studio(image_data, prompt)


# Instancia global
medical_analyzer = MedicalImageAnalysis()


async def analyze_image_with_fallback(image_data: str, image_format: str, prompt: str) -> Dict[str, Any]:
    """Funci√≥n helper para an√°lisis de imagen con LM Studio"""
    return await medical_analyzer.analyze_with_fallback(image_data, image_format, prompt)
